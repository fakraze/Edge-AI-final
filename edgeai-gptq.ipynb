{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-01T08:51:40.869420Z",
     "iopub.status.busy": "2025-06-01T08:51:40.869063Z",
     "iopub.status.idle": "2025-06-01T08:54:35.529231Z",
     "shell.execute_reply": "2025-06-01T08:54:35.527531Z",
     "shell.execute_reply.started": "2025-06-01T08:51:40.869376Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting scipy==1.11.4\n",
      "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\n",
      "Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.11.4\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.5.2\n",
      "    Uninstalling accelerate-1.5.2:\n",
      "      Successfully uninstalled accelerate-1.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.7.0 bitsandbytes-0.46.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.52.4\n",
      "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
      "Collecting gptqmodel\n",
      "  Downloading gptqmodel-2.2.0.tar.gz (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.3/284.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Running command python setup.py egg_info\n",
      "  [06/01/25 08:53:59] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: accelerate>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (1.7.0)\n",
      "Requirement already satisfied: datasets>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (1.26.4)\n",
      "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (2.6.0+cu124)\n",
      "Requirement already satisfied: safetensors>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.5.3)\n",
      "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (4.52.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (3.6.0)\n",
      "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (25.0)\n",
      "Collecting device-smi==0.4.1 (from gptqmodel)\n",
      "  Downloading device_smi-0.4.1.tar.gz (17 kB)\n",
      "  Running command python setup.py egg_info\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  [06/01/25 08:54:01] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting protobuf>=5.29.3 (from gptqmodel)\n",
      "  Obtaining dependency information for protobuf>=5.29.3 from https://files.pythonhosted.org/packages/fa/b1/b59d405d64d31999244643d88c45c8241c58f17cc887e73bcb90602327f8/protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (11.1.0)\n",
      "Requirement already satisfied: hf_transfer>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.1.9)\n",
      "Requirement already satisfied: huggingface_hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.31.1)\n",
      "Collecting random_word==1.0.13 (from gptqmodel)\n",
      "  Obtaining dependency information for random_word==1.0.13 from https://files.pythonhosted.org/packages/df/46/3ce7166b6f99e61fab286d807bf8834cb8e23adbc1becff047b37c9cd666/random_word-1.0.13-py3-none-any.whl.metadata\n",
      "  Downloading random_word-1.0.13-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting tokenicer==0.0.4 (from gptqmodel)\n",
      "  Downloading tokenicer-0.0.4.tar.gz (10 kB)\n",
      "  Running command python setup.py egg_info\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  [06/01/25 08:54:02] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting logbar==0.0.4 (from gptqmodel)\n",
      "  Downloading logbar-0.0.4.tar.gz (12 kB)\n",
      "  Running command python setup.py egg_info\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  [06/01/25 08:54:03] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting autopep8<3.0.0,>=2.3.1 (from random_word==1.0.13->gptqmodel)\n",
      "  Obtaining dependency information for autopep8<3.0.0,>=2.3.1 from https://files.pythonhosted.org/packages/9e/43/53afb8ba17218f19b77c7834128566c5bbb100a0ad9ba2e8e89d089d7079/autopep8-2.3.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading autopep8-2.3.2-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pytest<9.0.0,>=8.3.3 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel) (8.3.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel) (6.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel) (2.32.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.3.0->gptqmodel) (7.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.28.1->gptqmodel) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.28.1->gptqmodel) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->gptqmodel) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->gptqmodel) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (0.21.1)\n",
      "Collecting pycodestyle>=2.12.0 (from autopep8<3.0.0,>=2.3.1->random_word==1.0.13->gptqmodel)\n",
      "  Obtaining dependency information for pycodestyle>=2.12.0 from https://files.pythonhosted.org/packages/07/be/b00116df1bfb3e0bb5b45e29d604799f7b91dd861637e4d448b4e09e6a3e/pycodestyle-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (3.11.18)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random_word==1.0.13->gptqmodel) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random_word==1.0.13->gptqmodel) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->gptqmodel) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->gptqmodel) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->gptqmodel) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->gptqmodel) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->gptqmodel) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->gptqmodel) (1.20.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->gptqmodel) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.2.0->gptqmodel) (1.17.0)\n",
      "Downloading random_word-1.0.13-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autopep8-2.3.2-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycodestyle-2.13.0-py2.py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: gptqmodel, device-smi, logbar, tokenicer\n",
      "  Running command python setup.py bdist_wheel\n",
      "  Guessing wheel URL: https://github.com/ModelCloud/GPTQModel/releases/download/v2.2.0/gptqmodel-2.2.0+cpu-cp311-cp311-linux_x86_64.whl\n",
      "  wheel name=gptqmodel-2.2.0+cpu-cp311-cp311-linux_x86_64.whl\n",
      "  Precompiled wheel not found in url: https://github.com/ModelCloud/GPTQModel/releases/download/v2.2.0/gptqmodel-2.2.0+cpu-cp311-cp311-linux_x86_64.whl. Building from source...\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "  !!\n",
      "\n",
      "          ********************************************************************************\n",
      "          Please avoid running ``setup.py`` directly.\n",
      "          Instead, use pypa/build, pypa/installer or other\n",
      "          standards-based tools.\n",
      "\n",
      "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "          ********************************************************************************\n",
      "\n",
      "  !!\n",
      "    self.initialize_options()\n",
      "  [06/01/25 08:54:08] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Building wheel for gptqmodel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gptqmodel: filename=gptqmodel-2.2.0-py3-none-any.whl size=279960 sha256=72880d5b8263d8c7000c60a4bca72ceb56dd7f4efc8d08ac14038b4be61785e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/a0/10/12ae1ecaf1676096156195aeeea8ed4787c6eed58d3a00d140\n",
      "  Running command python setup.py bdist_wheel\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "  !!\n",
      "\n",
      "          ********************************************************************************\n",
      "          Please avoid running ``setup.py`` directly.\n",
      "          Instead, use pypa/build, pypa/installer or other\n",
      "          standards-based tools.\n",
      "\n",
      "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "          ********************************************************************************\n",
      "\n",
      "  !!\n",
      "    self.initialize_options()\n",
      "  [06/01/25 08:54:10] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Building wheel for device-smi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for device-smi: filename=device_smi-0.4.1-py3-none-any.whl size=17886 sha256=319dfc57f3ec40ad24b5ad48d3de75f2609776c7a65b2c8068b703b335f84e3f\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/82/37/d8751fd90fc238023fe6d5e3c7d7deedb07f03a3ed7264aca2\n",
      "  Running command python setup.py bdist_wheel\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "  !!\n",
      "\n",
      "          ********************************************************************************\n",
      "          Please avoid running ``setup.py`` directly.\n",
      "          Instead, use pypa/build, pypa/installer or other\n",
      "          standards-based tools.\n",
      "\n",
      "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "          ********************************************************************************\n",
      "\n",
      "  !!\n",
      "    self.initialize_options()\n",
      "  [06/01/25 08:54:11] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Building wheel for logbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for logbar: filename=logbar-0.0.4-py3-none-any.whl size=12635 sha256=02a84bee56115a1d234384ddf969bb64f23d3b5964407363fe1b198d104f3de1\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/0a/fc/2ee56584257132c0ea647e287239b349775e3fcc31c28b9d6c\n",
      "  Running command python setup.py bdist_wheel\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py:261: UserWarning: Unknown distribution option: 'platform'\n",
      "    warnings.warn(msg)\n",
      "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "  !!\n",
      "\n",
      "          ********************************************************************************\n",
      "          Please avoid running ``setup.py`` directly.\n",
      "          Instead, use pypa/build, pypa/installer or other\n",
      "          standards-based tools.\n",
      "\n",
      "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "          ********************************************************************************\n",
      "\n",
      "  !!\n",
      "    self.initialize_options()\n",
      "  [06/01/25 08:54:13] ERROR    listing git files failed - pretending there aren't any        git.py:26\n",
      "  Building wheel for tokenicer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tokenicer: filename=tokenicer-0.0.4-py3-none-any.whl size=11354 sha256=53425c86559857f13ff0c0591af0ca9b9294a150a264a9c73025214bf7ea44d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/72/04/60d66d0e3840894a498c4bc5536884b23bfbeb308308037d84\n",
      "Successfully built gptqmodel device-smi logbar tokenicer\n",
      "Installing collected packages: pycodestyle, protobuf, logbar, device-smi, autopep8, random_word, tokenicer, gptqmodel\n",
      "  changing mode of /usr/local/bin/pycodestyle to 755\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Removing file or directory /usr/local/lib/python3.11/dist-packages/google/protobuf/\n",
      "      Removing file or directory /usr/local/lib/python3.11/dist-packages/protobuf-3.20.3-nspkg.pth\n",
      "      Removing file or directory /usr/local/lib/python3.11/dist-packages/protobuf-3.20.3.dist-info/\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  changing mode of /usr/local/bin/autopep8 to 755\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "wandb 0.19.9 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 6.31.1 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-aiplatform 1.87.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed autopep8-2.3.2 device-smi-0.4.1 gptqmodel-2.2.0 logbar-0.0.4 protobuf-6.31.1 pycodestyle-2.13.0 random_word-1.0.13 tokenicer-0.0.4\n"
     ]
    }
   ],
   "source": [
    "# 從頭開始\n",
    "!pip install -U numpy==1.26.4 scipy==1.11.4\n",
    "!pip install -U transformers accelerate bitsandbytes tqdm datasets\n",
    "!pip install -v gptqmodel --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T08:51:20.884100Z",
     "iopub.status.busy": "2025-06-01T08:51:20.883663Z",
     "iopub.status.idle": "2025-06-01T08:51:21.764352Z",
     "shell.execute_reply": "2025-06-01T08:51:21.763129Z",
     "shell.execute_reply.started": "2025-06-01T08:51:20.884064Z"
    },
    "id": "ESfbOpMVieNQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")  # 用你的 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0141a02d79dc456790e29ae79410af8c",
      "3af040e849b34c428d13d82a14ce6e84",
      "a764c7a15e624f1a889e9dee4294b308",
      "818d1edd2cea4c85810b230bd8afa686",
      "0611c37987934b439d3e00b15ddbb48d",
      "f9b38c5c2bd942868816176b9478618b",
      "8093f8dc686a40d08fe8e533d279f5ab",
      "5b542d76d65049c48f680287cc673eda",
      "c2b97aefc59541608dd81e0acbe5878d",
      "b42f6fbc755c4b1b9ead57ab67ed2777",
      "c92b284a785649b5b06cba375571d751",
      "f50e536917f941ea80b601140d4dea4c",
      "19518e9f79a24bd081ee827dce8dffa6",
      "d76e0a196e7841b2b0fc98689cb1e0bb",
      "1464a3f1c2234b10b0325be3cde71053",
      "5d8c6c1d44b347548dd1c87744344825",
      "c3ff1d0a0f40447fbf241a1899e96116",
      "449674ec3b5245b69abde137ab334b6c",
      "aa9a3982b722474fa3a54ff6e1697b52",
      "dce72fadda9e48009e046a833ffb1403",
      "2d3761e3102d484a9cb1544e40c0e7b0",
      "21e5df46d0614f37a8991bc780821378",
      "dedacb59ff6e4aaeb0f4816cbd012fbc",
      "ad8191fc28134729a314834126b13a5a",
      "850f4ca3c069464c93bc4820051025e8",
      "44c975612e714ce987fafbbe3c7f2688",
      "7de12965cbe048749dc6e36bb19dbb49",
      "7e4401c368e24eb1b5728a6d80e9f5d0",
      "ef4311bf255a499a9b5901a07ce2a2ba",
      "d462f1607e4d481587974797a45ff720",
      "6b708577c7374eb79adb6172d87cc2f3",
      "ee4c49ea5c5845f385bd10ad305d6560",
      "c25935aaa83a483eb1726db9d3ef612e",
      "3f95878e25f64cc0938d8b03c14002a6",
      "100b37542725460eab99d1d71c63b5de",
      "f299e2151e4e45108e3591f9bcb6be46",
      "585e6bdee21d4c29a6205679fdbb0a14",
      "4fc80e2196164c0388005afad061dd75",
      "765b3effcbda4b3a89f1fccdf66d082d",
      "6b9b0ff72ac44773b59e418b4a1ab304",
      "138d8fdef75f47748a440b30f4462f10",
      "433f40199bc043bdb1448f39e8b9b54b",
      "d335c6dd74224a79a41892d40bc5bfbe",
      "9676fd43da0a4ed3bd6481b1afce908b",
      "90d6fba305be463689ef0ec0851c4754",
      "885c52398f9f45b19806eef976087847",
      "718c87180a6a40fe9d7d26c3e2658f67",
      "b83ef0305cee4e87ba4ab2215e2fa6a9",
      "81d053c5358f4c6999dd11358a43b61b",
      "65b24569d21247c68299a2312816eddf",
      "7f8e3e88b3dc46deb229e4865970a01c",
      "79013fa73a804f77ad02c52325ed20fd",
      "cdc5cb6acb8f458587f357cf410d4c66",
      "85964b240d3c4ce79795112d2de51bf1",
      "ba577dcfa9724c29b5fc03d70d86f68a",
      "abb460449f264eb2b5eff5668a4658df",
      "46982cb1f7374a5488378a1e494fe353",
      "c2175a08271a4a42a02760bd7df4418a",
      "6f88082c8b7643438c62a2750e1e541e",
      "5195aac8505c4a3b9a5c93b8c7d5557a",
      "2846c0f33e00431dbd7e87af31da8f91",
      "a9e3ebc2530e4252b3e0336eeb9d085a",
      "173a908967e04e748157a98a28839227",
      "ec167797c65a432d93962726051c1060",
      "555cd1ca0215456482373546d3880051",
      "b8a2263409c04e0a89e79c4c45a4b640",
      "5b1707d017d242e9949c2935c6df7a59",
      "3585db0fd71c414fac64fd45e65957b5",
      "270dea18d7844aa5b24685095dac128d",
      "1b456e4baae74877914590363d795724",
      "47469b8bcdf645b5a39c2a785649b1db",
      "4094a9c2fb8541be81d66d3a45ef6870",
      "290bf769a3bf4cc1ad9739b506e1b695",
      "e09b85a428ad48f7b681cfcefc52f821",
      "15c9ee01e4f84eac960c0ba22433052b",
      "43fb490f6f3c488e87f82679a04bbcb8",
      "7e6b2b45320c4a988dd87d02eafb61b5",
      "dc1f097040a3426b83ac3cdc69155678",
      "2b7b7c2bfbe644a0ac33859d089c1cd6",
      "cdd03a1ebca94cb7a86aadab871473c2",
      "e037bb7bf07047879ca5dd6537b81b86",
      "6cb19afba9664405b642b55a3b2f70f3",
      "27b6851d952a4fe193412622b740e887",
      "ca2c42e36a4f4098ad981c0c7c4df552",
      "2364abb8d3464eccb71417a6cd887ee3",
      "4623327d5c884c33be9fc12c2e5b839d",
      "d5cd1d3385534f95aaf96aee4aa9495a",
      "0e9a85f10252477fb6c668f486fe87e5",
      "b4fdc53dba884822b548ab16420bf2ad",
      "c854066d4f5c4aa4b6077aa8be6a819a",
      "f1285292c5114eae9da51727c1e7e83b",
      "3f51300e5602430080bb9560c395fed7",
      "f1b6b98c63a343aeab8f58bb38d18ed3",
      "acd05765bf7d4f4db44a82b3499e693c",
      "b8ab2eda5ee74a6a938db3152ab9396c",
      "ff3e956f1f044171b842f8a0ac91b776",
      "a8a6f97b17d34a0498364cee11dfaea8",
      "67fa155ad19544a7acfad297c37faf6c",
      "b4bf9704c1844367a2258ff4659b02da",
      "477aacb0fa074f3988c375e8ef5fad49",
      "539a9b6df5bd4806a6881ea61a4126ea",
      "26f170c80b0a401cac102aa286c764aa",
      "9a09db00f94142c29fd93bac265ffafb",
      "a14a7fa3d09a42b196db949d874130cd",
      "ef0eb6622dd54d41be39615660b29e5e",
      "b19cfc4707e14755a7311bb0928e227b",
      "31a048dcb3aa4e0fa8795d9ca18ca552",
      "be58c9aea387495f8f36d1857a05cbb7",
      "21ff6c8dc2fc4e77badc42f616415e70",
      "8d292fe2f97b43dba4162c363d5abf05",
      "d2637d151e464bc2a116fb8ed784e74d",
      "2332d534786c4832b9764e0cbb2c4040",
      "e6a5e9a37f9a4160a613bb9256272771",
      "c260485a020c4ab08191b9999bff5453",
      "ba73feb090c34e0980d0018d6dcaddcd",
      "f994ffd01c1e4e6ba3281f306166a55a",
      "9b07890e03a148478ba6e4136379237f",
      "826104cc9c404d80b71400976a73d9d7",
      "4085e2d1424f4c0d8420827744665cef",
      "f858c2356b0e460f812159eb0653e2bd",
      "f461efd65145423885a849af8362054c",
      "d89c0ca6480b4283be5235fc1ead7b22",
      "bac2296043814b7bb11d7890388e8a7c",
      "c8fffb29d7ee4dba8881b5f361c94010",
      "23b0735986244ad3ace09d01bb09527d",
      "90c6cdb6e801476eb75eb1171c79957d",
      "7b56c2a130044d519fb0069adf4631a4",
      "1275e7ca31d643a7b5e21f6e45ddd8ce",
      "cb78d9794fcd4984971867664127d91f",
      "b81939b9ede9430d950ebcba71037b12",
      "90e6f91f057e4325bac641a4c04c65b4",
      "682b1f34cd9746999c0aab6b739d8bf4"
     ]
    },
    "id": "UY2PzipMhg4V",
    "outputId": "f6d9b378-a583-429f-94c2-a18bc3002399",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# use QLoRA to do GPTQ\n",
    "\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 清空 PyTorch CUDA 記憶體\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# 清空 Python 變數參考\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# === Step 1: 載入原始模型 ===\n",
    "base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# === Step 2: 載入 QLoRA adapter 並合併 ===\n",
    "adapter_path = \"/kaggle/input/gptq-qlora2/qlora-wikitext2\"\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()  # 合併 adapter 權重\n",
    "\n",
    "# === Step 3: 儲存合併後模型 ===\n",
    "merged_path = \"/kaggle/working/merged-qlora-model\"\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "# # === Step 4: 準備 GPTQ 校準資料 ===\n",
    "# print(\"[4] Loading calibration dataset from huggingface wikitext-2...\")\n",
    "# raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "# calibration_dataset = [x[\"text\"] for x in raw_dataset if len(x[\"text\"].strip()) > 0][:128]\n",
    "\n",
    "# === Step 4: 準備 GPTQ 校準資料（提升到 1024 筆） ===\n",
    "print(\"[4] Loading calibration dataset from huggingface wikitext-2...\")\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "calibration_dataset = [x[\"text\"] for x in raw_dataset if len(x[\"text\"].strip()) > 0][:1024]\n",
    "\n",
    "\n",
    "# === Step 5: 進行 GPTQ 量化 ===\n",
    "print(\"[5] Quantizing model...\")\n",
    "quant_path = \"/kaggle/working/Qlora-GPTQModel-4bit\"\n",
    "quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "gptq_model = GPTQModel.load(merged_path, quant_config)\n",
    "gptq_model.quantize(calibration_dataset, batch_size=1)\n",
    "\n",
    "# ✅ 確認 kernel 是否成功啟用\n",
    "kernel = getattr(getattr(gptq_model.model, \"kernel\", None), \"name\", \"unknown\")\n",
    "print(\"[Kernel] GPTQ kernel used:\", kernel)\n",
    "\n",
    "# === Step 6: 儲存量化後模型 ===\n",
    "print(\"[6] Saving quantized model to:\", quant_path)\n",
    "gptq_model.save(quant_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-30T14:04:45.109048Z",
     "iopub.status.busy": "2025-05-30T14:04:45.108239Z",
     "iopub.status.idle": "2025-05-30T17:02:48.577766Z",
     "shell.execute_reply": "2025-05-30T17:02:48.576795Z",
     "shell.execute_reply.started": "2025-05-30T14:04:45.109011Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 14:04:54.589289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748613894.790853      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748613894.847926      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Loading calibration dataset (WikiText-2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142a45257803472588ac6023c32c311f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1724399e5b0a4d46b800162bf6659ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9774a8a52f894c55bc9014b62882312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640624f2bad542a99cdda46ec47714cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290311974400493b9e40c15c5232b778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfe6df90fb8428c953dbd82e7ac01a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814aa3b780164e4888e804e456fb28ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Loading model with GPTQModel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64616567cfcf491293bbad4cf169a196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb91a1eec444d10a1857de2afe0e966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be4dc7565c447c5a20c31adfcaad804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2389e1faca694f65a2968a02e9e58d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "USE_POLICY.md:   0%|          | 0.00/6.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a1f46564f84a049ced0bea62dc6e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7974b56170bd4f1489d59309ecfb6994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfc7d9ce1f8449c84e2cbf201d56b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ba8647a87946af8596a93eab927092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "orig_params.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a929d86e6fb4113b2f671cfaa5bbb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d3a5fb74ed42c9a23153fd1d8c30d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a046b502d9e4f538888dab78d44c178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cfab2c8b16418e8779bfbab21ad241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/consolidated.00.pth:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc57a47b0ec4001a10ac5cf76d930e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                                       \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff92676f9c849e1a4444a15ec28d589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                                       \n",
      "[5] Quantizing model...\n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`                       \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 99.41796875.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_unmoderated_time_05_30_2025_14h_06m_34s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.01435574\u001b[0m | 4096        | 0.01000     | 1.887     | 64.223       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00034006\u001b[0m | 4096        | 0.01000     | 1.131     | 64.223       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.02485722\u001b[0m | 4096        | 0.01000     | 1.444     | 64.223       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00003825\u001b[0m | 4096        | 0.01000     | 1.128     | 61.510       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[92m0.01627217\u001b[0m | 4096        | 0.01000     | 1.390     | 67.871       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[92m0.01822916\u001b[0m | 4096        | 0.01000     | 1.132     | 67.871       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.00021314\u001b[0m | 4096        | 0.01000     | 3.966     | 99.791       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.01253698\u001b[0m | 4096        | 0.01000     | 1.299     | 73.788       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.00105197\u001b[0m | 4096        | 0.01000     | 1.090     | 73.788       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.02101617\u001b[0m | 4096        | 0.01000     | 1.341     | 73.788       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00011428\u001b[0m | 4096        | 0.01000     | 1.147     | 62.934       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[92m0.02260970\u001b[0m | 4096        | 0.01000     | 1.256     | 68.411       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[92m0.02582929\u001b[0m | 4096        | 0.01000     | 1.431     | 68.411       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[96m0.16104259\u001b[0m | 4096        | 0.01000     | 3.750     | 100.033      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.06412478\u001b[0m | 4096        | 0.01000     | 1.457     | 73.959       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.00527728\u001b[0m | 4096        | 0.01000     | 1.334     | 73.959       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.09799074\u001b[0m | 4096        | 0.01000     | 1.170     | 73.959       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.00012938\u001b[0m | 4096        | 0.01000     | 1.266     | 63.089       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[92m0.03709250\u001b[0m | 4096        | 0.01000     | 1.168     | 68.347       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[92m0.04363991\u001b[0m | 4096        | 0.01000     | 1.183     | 68.347       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.00072039\u001b[0m | 4096        | 0.01000     | 3.939     | 99.642       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.05027297\u001b[0m | 4096        | 0.01000     | 1.457     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.00844509\u001b[0m | 4096        | 0.01000     | 1.386     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.08935129\u001b[0m | 4096        | 0.01000     | 1.265     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.00023333\u001b[0m | 4096        | 0.01000     | 1.340     | 63.300       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[92m0.04902439\u001b[0m | 4096        | 0.01000     | 1.216     | 68.481       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[92m0.06437659\u001b[0m | 4096        | 0.01000     | 1.478     | 68.481       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.00112029\u001b[0m | 4096        | 0.01000     | 4.185     | 99.596       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.04390191\u001b[0m | 4096        | 0.01000     | 1.098     | 73.941       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.00767628\u001b[0m | 4096        | 0.01000     | 1.190     | 73.941       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.08284789\u001b[0m | 4096        | 0.01000     | 1.104     | 73.941       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.00042364\u001b[0m | 4096        | 0.01000     | 1.154     | 63.102       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[92m0.05663256\u001b[0m | 4096        | 0.01000     | 1.365     | 68.236       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[92m0.08427033\u001b[0m | 4096        | 0.01000     | 1.289     | 68.236       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.00159266\u001b[0m | 4096        | 0.01000     | 3.682     | 99.937       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.07005687\u001b[0m | 4096        | 0.01000     | 1.385     | 73.832       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.00695181\u001b[0m | 4096        | 0.01000     | 1.180     | 73.832       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[96m0.11144952\u001b[0m | 4096        | 0.01000     | 1.259     | 73.832       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.00042795\u001b[0m | 4096        | 0.01000     | 1.158     | 63.113       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[92m0.06512734\u001b[0m | 4096        | 0.01000     | 1.382     | 68.221       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[92m0.09120330\u001b[0m | 4096        | 0.01000     | 1.226     | 68.221       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.00209223\u001b[0m | 4096        | 0.01000     | 4.049     | 100.017      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.05485114\u001b[0m | 4096        | 0.01000     | 1.359     | 73.693       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.00744243\u001b[0m | 4096        | 0.01000     | 1.090     | 73.693       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[96m0.10061929\u001b[0m | 4096        | 0.01000     | 1.160     | 73.693       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.00069538\u001b[0m | 4096        | 0.01000     | 1.097     | 62.900       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[92m0.06817757\u001b[0m | 4096        | 0.01000     | 1.439     | 68.048       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[92m0.09416599\u001b[0m | 4096        | 0.01000     | 1.232     | 68.048       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.00239365\u001b[0m | 4096        | 0.01000     | 4.021     | 99.763       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.05263212\u001b[0m | 4096        | 0.01000     | 1.218     | 73.874       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.00719871\u001b[0m | 4096        | 0.01000     | 1.124     | 73.874       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.08736084\u001b[0m | 4096        | 0.01000     | 1.130     | 73.874       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.00095008\u001b[0m | 4096        | 0.01000     | 1.342     | 63.153       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[92m0.07024772\u001b[0m | 4096        | 0.01000     | 1.283     | 68.423       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[92m0.08959071\u001b[0m | 4096        | 0.01000     | 1.391     | 68.423       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.00262369\u001b[0m | 4096        | 0.01000     | 3.990     | 100.065      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.06234256\u001b[0m | 4096        | 0.01000     | 1.163     | 73.785       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.00833968\u001b[0m | 4096        | 0.01000     | 1.446     | 73.785       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[96m0.10456616\u001b[0m | 4096        | 0.01000     | 1.340     | 73.785       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.00118713\u001b[0m | 4096        | 0.01000     | 1.420     | 62.996       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[92m0.07279390\u001b[0m | 4096        | 0.01000     | 1.190     | 68.397       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[92m0.09432168\u001b[0m | 4096        | 0.01000     | 1.289     | 68.397       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.00281574\u001b[0m | 4096        | 0.01000     | 3.958     | 99.973       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.06156176\u001b[0m | 4096        | 0.01000     | 1.108     | 73.572       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.01078480\u001b[0m | 4096        | 0.01000     | 1.244     | 73.572       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[96m0.10385101\u001b[0m | 4096        | 0.01000     | 1.177     | 73.572       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.00126495\u001b[0m | 4096        | 0.01000     | 1.075     | 62.798       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[92m0.07342093\u001b[0m | 4096        | 0.01000     | 1.295     | 68.086       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[92m0.09140582\u001b[0m | 4096        | 0.01000     | 1.360     | 68.086       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.00274293\u001b[0m | 4096        | 0.01000     | 3.745     | 99.144       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.06141173\u001b[0m | 4096        | 0.01000     | 1.094     | 74.244       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.00810589\u001b[0m | 4096        | 0.01000     | 1.285     | 74.244       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[96m0.10137618\u001b[0m | 4096        | 0.01000     | 1.254     | 74.244       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.00101643\u001b[0m | 4096        | 0.01000     | 1.126     | 63.140       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[92m0.07490519\u001b[0m | 4096        | 0.01000     | 1.159     | 68.387       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[92m0.08711967\u001b[0m | 4096        | 0.01000     | 1.277     | 68.387       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.00291804\u001b[0m | 4096        | 0.01000     | 3.914     | 100.028      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.04958594\u001b[0m | 4096        | 0.01000     | 1.417     | 74.097       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.01027959\u001b[0m | 4096        | 0.01000     | 1.319     | 74.097       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.08842330\u001b[0m | 4096        | 0.01000     | 1.408     | 74.097       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.00138187\u001b[0m | 4096        | 0.01000     | 1.304     | 63.277       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[92m0.07566598\u001b[0m | 4096        | 0.01000     | 1.449     | 68.413       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[92m0.08412928\u001b[0m | 4096        | 0.01000     | 1.129     | 68.413       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.00316336\u001b[0m | 4096        | 0.01000     | 3.911     | 100.424      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[92m0.06554198\u001b[0m | 4096        | 0.01000     | 1.350     | 74.156       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.00997160\u001b[0m | 4096        | 0.01000     | 1.125     | 74.156       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[96m0.11080000\u001b[0m | 4096        | 0.01000     | 1.184     | 74.156       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.00135277\u001b[0m | 4096        | 0.01000     | 1.093     | 63.180       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[92m0.07750727\u001b[0m | 4096        | 0.01000     | 1.147     | 68.466       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[92m0.08473474\u001b[0m | 4096        | 0.01000     | 1.320     | 68.466       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.00331091\u001b[0m | 4096        | 0.01000     | 3.868     | 100.253      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[92m0.06765112\u001b[0m | 4096        | 0.01000     | 1.193     | 74.051       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.01065525\u001b[0m | 4096        | 0.01000     | 1.144     | 74.051       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[96m0.10541699\u001b[0m | 4096        | 0.01000     | 1.094     | 74.051       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.00161704\u001b[0m | 4096        | 0.01000     | 1.274     | 63.151       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[92m0.08248083\u001b[0m | 4096        | 0.01000     | 1.195     | 68.401       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[92m0.09405470\u001b[0m | 4096        | 0.01000     | 1.352     | 68.401       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[92m0.00424911\u001b[0m | 4096        | 0.01000     | 3.827     | 100.088      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[92m0.05984748\u001b[0m | 4096        | 0.01000     | 1.198     | 73.797       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[92m0.01265368\u001b[0m | 4096        | 0.01000     | 1.287     | 73.797       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[96m0.12171478\u001b[0m | 4096        | 0.01000     | 1.152     | 73.797       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.00204706\u001b[0m | 4096        | 0.01000     | 1.092     | 62.966       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[92m0.08927198\u001b[0m | 4096        | 0.01000     | 1.345     | 68.029       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[96m0.10355449\u001b[0m | 4096        | 0.01000     | 1.163     | 68.029       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[92m0.00527470\u001b[0m | 4096        | 0.01000     | 3.705     | 99.722       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[92m0.06772700\u001b[0m | 4096        | 0.01000     | 1.203     | 73.776       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[92m0.01283312\u001b[0m | 4096        | 0.01000     | 1.082     | 73.776       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[96m0.12394813\u001b[0m | 4096        | 0.01000     | 1.089     | 73.776       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[92m0.00124206\u001b[0m | 4096        | 0.01000     | 1.410     | 63.224       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[92m0.09151001\u001b[0m | 4096        | 0.01000     | 1.256     | 68.388       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[96m0.11611484\u001b[0m | 4096        | 0.01000     | 1.166     | 68.388       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[92m0.00562578\u001b[0m | 4096        | 0.01000     | 4.030     | 100.114      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[92m0.07749595\u001b[0m | 4096        | 0.01000     | 1.177     | 73.633       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[92m0.01426380\u001b[0m | 4096        | 0.01000     | 1.219     | 73.633       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[96m0.12895831\u001b[0m | 4096        | 0.01000     | 1.415     | 73.633       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[92m0.00096435\u001b[0m | 4096        | 0.01000     | 1.332     | 62.955       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[92m0.09566607\u001b[0m | 4096        | 0.01000     | 1.343     | 68.176       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[96m0.12527244\u001b[0m | 4096        | 0.01000     | 1.145     | 68.176       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[92m0.00546586\u001b[0m | 4096        | 0.01000     | 4.219     | 99.454       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[92m0.07350655\u001b[0m | 4096        | 0.01000     | 1.107     | 74.183       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[92m0.01476912\u001b[0m | 4096        | 0.01000     | 1.071     | 74.183       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[96m0.12994026\u001b[0m | 4096        | 0.01000     | 1.083     | 74.183       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[92m0.00081307\u001b[0m | 4096        | 0.01000     | 1.134     | 63.247       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[96m0.10132734\u001b[0m | 4096        | 0.01000     | 1.304     | 68.510       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[96m0.13465160\u001b[0m | 4096        | 0.01000     | 1.201     | 68.510       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[92m0.00593460\u001b[0m | 4096        | 0.01000     | 3.801     | 100.100      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[92m0.08168524\u001b[0m | 4096        | 0.01000     | 1.165     | 73.855       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[92m0.01682745\u001b[0m | 4096        | 0.01000     | 1.090     | 73.855       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[96m0.13980797\u001b[0m | 4096        | 0.01000     | 1.081     | 73.855       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[92m0.00080897\u001b[0m | 4096        | 0.01000     | 1.381     | 63.189       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[96m0.11095390\u001b[0m | 4096        | 0.01000     | 1.271     | 68.303       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[96m0.14529637\u001b[0m | 4096        | 0.01000     | 1.186     | 68.303       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[92m0.00645607\u001b[0m | 4096        | 0.01000     | 3.758     | 99.996       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[92m0.08657898\u001b[0m | 4096        | 0.01000     | 1.386     | 73.735       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[92m0.01847691\u001b[0m | 4096        | 0.01000     | 1.370     | 73.735       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[96m0.13750736\u001b[0m | 4096        | 0.01000     | 1.199     | 73.735       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[92m0.00136538\u001b[0m | 4096        | 0.01000     | 1.443     | 63.063       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[96m0.12176090\u001b[0m | 4096        | 0.01000     | 1.301     | 68.165       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[96m0.15609980\u001b[0m | 4096        | 0.01000     | 1.347     | 68.165       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[92m0.00784970\u001b[0m | 4096        | 0.01000     | 3.935     | 99.897       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[92m0.08765212\u001b[0m | 4096        | 0.01000     | 1.392     | 73.929       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[92m0.02263182\u001b[0m | 4096        | 0.01000     | 1.104     | 73.929       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[96m0.14400068\u001b[0m | 4096        | 0.01000     | 1.081     | 73.929       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[92m0.00113469\u001b[0m | 4096        | 0.01000     | 1.113     | 62.892       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[96m0.12822641\u001b[0m | 4096        | 0.01000     | 1.447     | 68.121       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[96m0.15817228\u001b[0m | 4096        | 0.01000     | 1.391     | 68.121       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[92m0.00781691\u001b[0m | 4096        | 0.01000     | 4.460     | 99.989       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[92m0.08616440\u001b[0m | 4096        | 0.01000     | 1.291     | 73.930       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[92m0.02839615\u001b[0m | 4096        | 0.01000     | 1.103     | 73.930       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[96m0.14385091\u001b[0m | 4096        | 0.01000     | 1.103     | 73.930       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[92m0.00121116\u001b[0m | 4096        | 0.01000     | 1.090     | 62.946       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[96m0.13826860\u001b[0m | 4096        | 0.01000     | 1.205     | 68.449       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[96m0.17151400\u001b[0m | 4096        | 0.01000     | 1.205     | 68.449       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[92m0.00856496\u001b[0m | 4096        | 0.01000     | 4.038     | 99.879       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[92m0.08290993\u001b[0m | 4096        | 0.01000     | 1.201     | 73.857       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[92m0.02976696\u001b[0m | 4096        | 0.01000     | 1.132     | 73.857       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[96m0.14884555\u001b[0m | 4096        | 0.01000     | 1.233     | 73.857       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.o_proj     | \u001b[92m0.00122636\u001b[0m | 4096        | 0.01000     | 1.106     | 62.843       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.up_proj          | \u001b[96m0.15146972\u001b[0m | 4096        | 0.01000     | 1.231     | 68.441       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.gate_proj        | \u001b[96m0.18892969\u001b[0m | 4096        | 0.01000     | 1.117     | 68.441       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.down_proj        | \u001b[92m0.00983447\u001b[0m | 4096        | 0.01000     | 3.921     | 99.758       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[92m0.09149595\u001b[0m | 4096        | 0.01000     | 1.512     | 73.937       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[92m0.02792757\u001b[0m | 4096        | 0.01000     | 1.310     | 73.937       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[96m0.14742397\u001b[0m | 4096        | 0.01000     | 1.460     | 73.937       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.o_proj     | \u001b[92m0.00205535\u001b[0m | 4096        | 0.01000     | 1.394     | 63.090       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.up_proj          | \u001b[96m0.16773717\u001b[0m | 4096        | 0.01000     | 1.165     | 68.435       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.gate_proj        | \u001b[96m0.21768546\u001b[0m | 4096        | 0.01000     | 1.219     | 68.435       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.down_proj        | \u001b[92m0.01152225\u001b[0m | 4096        | 0.01000     | 3.544     | 99.899       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.k_proj     | \u001b[96m0.10135132\u001b[0m | 4096        | 0.01000     | 1.443     | 73.872       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.v_proj     | \u001b[92m0.04113655\u001b[0m | 4096        | 0.01000     | 1.314     | 73.872       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.q_proj     | \u001b[96m0.16089720\u001b[0m | 4096        | 0.01000     | 1.275     | 73.872       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.o_proj     | \u001b[92m0.00307318\u001b[0m | 4096        | 0.01000     | 1.362     | 63.030       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.up_proj          | \u001b[96m0.18329039\u001b[0m | 4096        | 0.01000     | 1.213     | 68.198       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.gate_proj        | \u001b[96m0.24332288\u001b[0m | 4096        | 0.01000     | 1.322     | 68.198       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.down_proj        | \u001b[92m0.01328651\u001b[0m | 4096        | 0.01000     | 3.595     | 99.900       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.k_proj     | \u001b[92m0.08178437\u001b[0m | 4096        | 0.01000     | 1.191     | 73.759       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.v_proj     | \u001b[92m0.03918345\u001b[0m | 4096        | 0.01000     | 1.169     | 73.759       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.q_proj     | \u001b[96m0.16061834\u001b[0m | 4096        | 0.01000     | 1.405     | 73.759       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | self_attn.o_proj     | \u001b[92m0.00337070\u001b[0m | 4096        | 0.01000     | 1.154     | 62.867       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.up_proj          | \u001b[96m0.20346707\u001b[0m | 4096        | 0.01000     | 1.263     | 68.212       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.gate_proj        | \u001b[96m0.26730213\u001b[0m | 4096        | 0.01000     | 1.344     | 68.212       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 25        | mlp.down_proj        | \u001b[92m0.01755981\u001b[0m | 4096        | 0.01000     | 3.770     | 99.051       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.k_proj     | \u001b[92m0.08656490\u001b[0m | 4096        | 0.01000     | 1.352     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.v_proj     | \u001b[92m0.04972475\u001b[0m | 4096        | 0.01000     | 1.370     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.q_proj     | \u001b[96m0.14920008\u001b[0m | 4096        | 0.01000     | 1.260     | 74.069       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | self_attn.o_proj     | \u001b[92m0.00657041\u001b[0m | 4096        | 0.01000     | 1.087     | 63.091       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.up_proj          | \u001b[96m0.21160695\u001b[0m | 4096        | 0.01000     | 1.309     | 68.432       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.gate_proj        | \u001b[96m0.28265557\u001b[0m | 4096        | 0.01000     | 1.215     | 68.432       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 26        | mlp.down_proj        | \u001b[92m0.02299990\u001b[0m | 4096        | 0.01000     | 3.563     | 99.554       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.k_proj     | \u001b[92m0.06566277\u001b[0m | 4096        | 0.01000     | 1.472     | 74.096       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.v_proj     | \u001b[92m0.03486685\u001b[0m | 4096        | 0.01000     | 1.094     | 74.096       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.q_proj     | \u001b[96m0.12184440\u001b[0m | 4096        | 0.01000     | 1.107     | 74.096       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | self_attn.o_proj     | \u001b[92m0.01116214\u001b[0m | 4096        | 0.01000     | 1.327     | 63.057       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.up_proj          | \u001b[96m0.22326216\u001b[0m | 4096        | 0.01000     | 1.179     | 68.472       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.gate_proj        | \u001b[96m0.26846239\u001b[0m | 4096        | 0.01000     | 1.200     | 68.472       | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`    \n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 27        | mlp.down_proj        | \u001b[92m0.00556553\u001b[0m | 4096        | 0.01250     | 4.532     | 100.122      | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.01435574', 'samples': '4096', 'damp': '0.01000', 'time': '1.887', 'fwd_time': '64.223'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00034006', 'samples': '4096', 'damp': '0.01000', 'time': '1.131', 'fwd_time': '64.223'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.02485722', 'samples': '4096', 'damp': '0.01000', 'time': '1.444', 'fwd_time': '64.223'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00003825', 'samples': '4096', 'damp': '0.01000', 'time': '1.128', 'fwd_time': '61.510'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.01627217', 'samples': '4096', 'damp': '0.01000', 'time': '1.390', 'fwd_time': '67.871'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.01822916', 'samples': '4096', 'damp': '0.01000', 'time': '1.132', 'fwd_time': '67.871'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00021314', 'samples': '4096', 'damp': '0.01000', 'time': '3.966', 'fwd_time': '99.791'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.01253698', 'samples': '4096', 'damp': '0.01000', 'time': '1.299', 'fwd_time': '73.788'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.00105197', 'samples': '4096', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '73.788'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.02101617', 'samples': '4096', 'damp': '0.01000', 'time': '1.341', 'fwd_time': '73.788'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00011428', 'samples': '4096', 'damp': '0.01000', 'time': '1.147', 'fwd_time': '62.934'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.02260970', 'samples': '4096', 'damp': '0.01000', 'time': '1.256', 'fwd_time': '68.411'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.02582929', 'samples': '4096', 'damp': '0.01000', 'time': '1.431', 'fwd_time': '68.411'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.16104259', 'samples': '4096', 'damp': '0.01000', 'time': '3.750', 'fwd_time': '100.033'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.06412478', 'samples': '4096', 'damp': '0.01000', 'time': '1.457', 'fwd_time': '73.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.00527728', 'samples': '4096', 'damp': '0.01000', 'time': '1.334', 'fwd_time': '73.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.09799074', 'samples': '4096', 'damp': '0.01000', 'time': '1.170', 'fwd_time': '73.959'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00012938', 'samples': '4096', 'damp': '0.01000', 'time': '1.266', 'fwd_time': '63.089'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.03709250', 'samples': '4096', 'damp': '0.01000', 'time': '1.168', 'fwd_time': '68.347'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.04363991', 'samples': '4096', 'damp': '0.01000', 'time': '1.183', 'fwd_time': '68.347'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.00072039', 'samples': '4096', 'damp': '0.01000', 'time': '3.939', 'fwd_time': '99.642'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.05027297', 'samples': '4096', 'damp': '0.01000', 'time': '1.457', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.00844509', 'samples': '4096', 'damp': '0.01000', 'time': '1.386', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.08935129', 'samples': '4096', 'damp': '0.01000', 'time': '1.265', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00023333', 'samples': '4096', 'damp': '0.01000', 'time': '1.340', 'fwd_time': '63.300'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.04902439', 'samples': '4096', 'damp': '0.01000', 'time': '1.216', 'fwd_time': '68.481'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.06437659', 'samples': '4096', 'damp': '0.01000', 'time': '1.478', 'fwd_time': '68.481'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.00112029', 'samples': '4096', 'damp': '0.01000', 'time': '4.185', 'fwd_time': '99.596'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.04390191', 'samples': '4096', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '73.941'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.00767628', 'samples': '4096', 'damp': '0.01000', 'time': '1.190', 'fwd_time': '73.941'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.08284789', 'samples': '4096', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '73.941'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00042364', 'samples': '4096', 'damp': '0.01000', 'time': '1.154', 'fwd_time': '63.102'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.05663256', 'samples': '4096', 'damp': '0.01000', 'time': '1.365', 'fwd_time': '68.236'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.08427033', 'samples': '4096', 'damp': '0.01000', 'time': '1.289', 'fwd_time': '68.236'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.00159266', 'samples': '4096', 'damp': '0.01000', 'time': '3.682', 'fwd_time': '99.937'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.07005687', 'samples': '4096', 'damp': '0.01000', 'time': '1.385', 'fwd_time': '73.832'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.00695181', 'samples': '4096', 'damp': '0.01000', 'time': '1.180', 'fwd_time': '73.832'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.11144952', 'samples': '4096', 'damp': '0.01000', 'time': '1.259', 'fwd_time': '73.832'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.00042795', 'samples': '4096', 'damp': '0.01000', 'time': '1.158', 'fwd_time': '63.113'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.06512734', 'samples': '4096', 'damp': '0.01000', 'time': '1.382', 'fwd_time': '68.221'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.09120330', 'samples': '4096', 'damp': '0.01000', 'time': '1.226', 'fwd_time': '68.221'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.00209223', 'samples': '4096', 'damp': '0.01000', 'time': '4.049', 'fwd_time': '100.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.05485114', 'samples': '4096', 'damp': '0.01000', 'time': '1.359', 'fwd_time': '73.693'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.00744243', 'samples': '4096', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '73.693'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.10061929', 'samples': '4096', 'damp': '0.01000', 'time': '1.160', 'fwd_time': '73.693'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.00069538', 'samples': '4096', 'damp': '0.01000', 'time': '1.097', 'fwd_time': '62.900'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.06817757', 'samples': '4096', 'damp': '0.01000', 'time': '1.439', 'fwd_time': '68.048'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.09416599', 'samples': '4096', 'damp': '0.01000', 'time': '1.232', 'fwd_time': '68.048'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.00239365', 'samples': '4096', 'damp': '0.01000', 'time': '4.021', 'fwd_time': '99.763'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.05263212', 'samples': '4096', 'damp': '0.01000', 'time': '1.218', 'fwd_time': '73.874'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.00719871', 'samples': '4096', 'damp': '0.01000', 'time': '1.124', 'fwd_time': '73.874'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.08736084', 'samples': '4096', 'damp': '0.01000', 'time': '1.130', 'fwd_time': '73.874'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.00095008', 'samples': '4096', 'damp': '0.01000', 'time': '1.342', 'fwd_time': '63.153'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.07024772', 'samples': '4096', 'damp': '0.01000', 'time': '1.283', 'fwd_time': '68.423'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.08959071', 'samples': '4096', 'damp': '0.01000', 'time': '1.391', 'fwd_time': '68.423'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.00262369', 'samples': '4096', 'damp': '0.01000', 'time': '3.990', 'fwd_time': '100.065'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.06234256', 'samples': '4096', 'damp': '0.01000', 'time': '1.163', 'fwd_time': '73.785'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.00833968', 'samples': '4096', 'damp': '0.01000', 'time': '1.446', 'fwd_time': '73.785'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.10456616', 'samples': '4096', 'damp': '0.01000', 'time': '1.340', 'fwd_time': '73.785'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.00118713', 'samples': '4096', 'damp': '0.01000', 'time': '1.420', 'fwd_time': '62.996'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.07279390', 'samples': '4096', 'damp': '0.01000', 'time': '1.190', 'fwd_time': '68.397'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '0.09432168', 'samples': '4096', 'damp': '0.01000', 'time': '1.289', 'fwd_time': '68.397'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.00281574', 'samples': '4096', 'damp': '0.01000', 'time': '3.958', 'fwd_time': '99.973'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.06156176', 'samples': '4096', 'damp': '0.01000', 'time': '1.108', 'fwd_time': '73.572'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.01078480', 'samples': '4096', 'damp': '0.01000', 'time': '1.244', 'fwd_time': '73.572'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.10385101', 'samples': '4096', 'damp': '0.01000', 'time': '1.177', 'fwd_time': '73.572'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.00126495', 'samples': '4096', 'damp': '0.01000', 'time': '1.075', 'fwd_time': '62.798'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.07342093', 'samples': '4096', 'damp': '0.01000', 'time': '1.295', 'fwd_time': '68.086'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '0.09140582', 'samples': '4096', 'damp': '0.01000', 'time': '1.360', 'fwd_time': '68.086'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.00274293', 'samples': '4096', 'damp': '0.01000', 'time': '3.745', 'fwd_time': '99.144'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.06141173', 'samples': '4096', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '74.244'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.00810589', 'samples': '4096', 'damp': '0.01000', 'time': '1.285', 'fwd_time': '74.244'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.10137618', 'samples': '4096', 'damp': '0.01000', 'time': '1.254', 'fwd_time': '74.244'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.00101643', 'samples': '4096', 'damp': '0.01000', 'time': '1.126', 'fwd_time': '63.140'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '0.07490519', 'samples': '4096', 'damp': '0.01000', 'time': '1.159', 'fwd_time': '68.387'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '0.08711967', 'samples': '4096', 'damp': '0.01000', 'time': '1.277', 'fwd_time': '68.387'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.00291804', 'samples': '4096', 'damp': '0.01000', 'time': '3.914', 'fwd_time': '100.028'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.04958594', 'samples': '4096', 'damp': '0.01000', 'time': '1.417', 'fwd_time': '74.097'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.01027959', 'samples': '4096', 'damp': '0.01000', 'time': '1.319', 'fwd_time': '74.097'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.08842330', 'samples': '4096', 'damp': '0.01000', 'time': '1.408', 'fwd_time': '74.097'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.00138187', 'samples': '4096', 'damp': '0.01000', 'time': '1.304', 'fwd_time': '63.277'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '0.07566598', 'samples': '4096', 'damp': '0.01000', 'time': '1.449', 'fwd_time': '68.413'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '0.08412928', 'samples': '4096', 'damp': '0.01000', 'time': '1.129', 'fwd_time': '68.413'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.00316336', 'samples': '4096', 'damp': '0.01000', 'time': '3.911', 'fwd_time': '100.424'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.06554198', 'samples': '4096', 'damp': '0.01000', 'time': '1.350', 'fwd_time': '74.156'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.00997160', 'samples': '4096', 'damp': '0.01000', 'time': '1.125', 'fwd_time': '74.156'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.11080000', 'samples': '4096', 'damp': '0.01000', 'time': '1.184', 'fwd_time': '74.156'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.00135277', 'samples': '4096', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '63.180'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '0.07750727', 'samples': '4096', 'damp': '0.01000', 'time': '1.147', 'fwd_time': '68.466'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '0.08473474', 'samples': '4096', 'damp': '0.01000', 'time': '1.320', 'fwd_time': '68.466'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.00331091', 'samples': '4096', 'damp': '0.01000', 'time': '3.868', 'fwd_time': '100.253'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.06765112', 'samples': '4096', 'damp': '0.01000', 'time': '1.193', 'fwd_time': '74.051'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.01065525', 'samples': '4096', 'damp': '0.01000', 'time': '1.144', 'fwd_time': '74.051'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.10541699', 'samples': '4096', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '74.051'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.00161704', 'samples': '4096', 'damp': '0.01000', 'time': '1.274', 'fwd_time': '63.151'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '0.08248083', 'samples': '4096', 'damp': '0.01000', 'time': '1.195', 'fwd_time': '68.401'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '0.09405470', 'samples': '4096', 'damp': '0.01000', 'time': '1.352', 'fwd_time': '68.401'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.00424911', 'samples': '4096', 'damp': '0.01000', 'time': '3.827', 'fwd_time': '100.088'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.05984748', 'samples': '4096', 'damp': '0.01000', 'time': '1.198', 'fwd_time': '73.797'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.01265368', 'samples': '4096', 'damp': '0.01000', 'time': '1.287', 'fwd_time': '73.797'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.12171478', 'samples': '4096', 'damp': '0.01000', 'time': '1.152', 'fwd_time': '73.797'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.00204706', 'samples': '4096', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '62.966'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '0.08927198', 'samples': '4096', 'damp': '0.01000', 'time': '1.345', 'fwd_time': '68.029'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '0.10355449', 'samples': '4096', 'damp': '0.01000', 'time': '1.163', 'fwd_time': '68.029'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.00527470', 'samples': '4096', 'damp': '0.01000', 'time': '3.705', 'fwd_time': '99.722'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.06772700', 'samples': '4096', 'damp': '0.01000', 'time': '1.203', 'fwd_time': '73.776'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.01283312', 'samples': '4096', 'damp': '0.01000', 'time': '1.082', 'fwd_time': '73.776'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.12394813', 'samples': '4096', 'damp': '0.01000', 'time': '1.089', 'fwd_time': '73.776'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.00124206', 'samples': '4096', 'damp': '0.01000', 'time': '1.410', 'fwd_time': '63.224'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '0.09151001', 'samples': '4096', 'damp': '0.01000', 'time': '1.256', 'fwd_time': '68.388'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '0.11611484', 'samples': '4096', 'damp': '0.01000', 'time': '1.166', 'fwd_time': '68.388'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.00562578', 'samples': '4096', 'damp': '0.01000', 'time': '4.030', 'fwd_time': '100.114'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.07749595', 'samples': '4096', 'damp': '0.01000', 'time': '1.177', 'fwd_time': '73.633'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.01426380', 'samples': '4096', 'damp': '0.01000', 'time': '1.219', 'fwd_time': '73.633'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.12895831', 'samples': '4096', 'damp': '0.01000', 'time': '1.415', 'fwd_time': '73.633'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.00096435', 'samples': '4096', 'damp': '0.01000', 'time': '1.332', 'fwd_time': '62.955'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '0.09566607', 'samples': '4096', 'damp': '0.01000', 'time': '1.343', 'fwd_time': '68.176'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '0.12527244', 'samples': '4096', 'damp': '0.01000', 'time': '1.145', 'fwd_time': '68.176'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.00546586', 'samples': '4096', 'damp': '0.01000', 'time': '4.219', 'fwd_time': '99.454'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.07350655', 'samples': '4096', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '74.183'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.01476912', 'samples': '4096', 'damp': '0.01000', 'time': '1.071', 'fwd_time': '74.183'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.12994026', 'samples': '4096', 'damp': '0.01000', 'time': '1.083', 'fwd_time': '74.183'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.00081307', 'samples': '4096', 'damp': '0.01000', 'time': '1.134', 'fwd_time': '63.247'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '0.10132734', 'samples': '4096', 'damp': '0.01000', 'time': '1.304', 'fwd_time': '68.510'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '0.13465160', 'samples': '4096', 'damp': '0.01000', 'time': '1.201', 'fwd_time': '68.510'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.00593460', 'samples': '4096', 'damp': '0.01000', 'time': '3.801', 'fwd_time': '100.100'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.08168524', 'samples': '4096', 'damp': '0.01000', 'time': '1.165', 'fwd_time': '73.855'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.01682745', 'samples': '4096', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '73.855'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.13980797', 'samples': '4096', 'damp': '0.01000', 'time': '1.081', 'fwd_time': '73.855'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.00080897', 'samples': '4096', 'damp': '0.01000', 'time': '1.381', 'fwd_time': '63.189'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '0.11095390', 'samples': '4096', 'damp': '0.01000', 'time': '1.271', 'fwd_time': '68.303'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '0.14529637', 'samples': '4096', 'damp': '0.01000', 'time': '1.186', 'fwd_time': '68.303'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '0.00645607', 'samples': '4096', 'damp': '0.01000', 'time': '3.758', 'fwd_time': '99.996'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.08657898', 'samples': '4096', 'damp': '0.01000', 'time': '1.386', 'fwd_time': '73.735'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.01847691', 'samples': '4096', 'damp': '0.01000', 'time': '1.370', 'fwd_time': '73.735'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.13750736', 'samples': '4096', 'damp': '0.01000', 'time': '1.199', 'fwd_time': '73.735'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.00136538', 'samples': '4096', 'damp': '0.01000', 'time': '1.443', 'fwd_time': '63.063'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '0.12176090', 'samples': '4096', 'damp': '0.01000', 'time': '1.301', 'fwd_time': '68.165'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '0.15609980', 'samples': '4096', 'damp': '0.01000', 'time': '1.347', 'fwd_time': '68.165'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '0.00784970', 'samples': '4096', 'damp': '0.01000', 'time': '3.935', 'fwd_time': '99.897'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.08765212', 'samples': '4096', 'damp': '0.01000', 'time': '1.392', 'fwd_time': '73.929'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.02263182', 'samples': '4096', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '73.929'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.14400068', 'samples': '4096', 'damp': '0.01000', 'time': '1.081', 'fwd_time': '73.929'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.00113469', 'samples': '4096', 'damp': '0.01000', 'time': '1.113', 'fwd_time': '62.892'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '0.12822641', 'samples': '4096', 'damp': '0.01000', 'time': '1.447', 'fwd_time': '68.121'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '0.15817228', 'samples': '4096', 'damp': '0.01000', 'time': '1.391', 'fwd_time': '68.121'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '0.00781691', 'samples': '4096', 'damp': '0.01000', 'time': '4.460', 'fwd_time': '99.989'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.08616440', 'samples': '4096', 'damp': '0.01000', 'time': '1.291', 'fwd_time': '73.930'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.02839615', 'samples': '4096', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '73.930'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.14385091', 'samples': '4096', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '73.930'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.00121116', 'samples': '4096', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '62.946'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '0.13826860', 'samples': '4096', 'damp': '0.01000', 'time': '1.205', 'fwd_time': '68.449'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '0.17151400', 'samples': '4096', 'damp': '0.01000', 'time': '1.205', 'fwd_time': '68.449'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '0.00856496', 'samples': '4096', 'damp': '0.01000', 'time': '4.038', 'fwd_time': '99.879'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '0.08290993', 'samples': '4096', 'damp': '0.01000', 'time': '1.201', 'fwd_time': '73.857'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '0.02976696', 'samples': '4096', 'damp': '0.01000', 'time': '1.132', 'fwd_time': '73.857'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '0.14884555', 'samples': '4096', 'damp': '0.01000', 'time': '1.233', 'fwd_time': '73.857'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.o_proj', 'loss': '0.00122636', 'samples': '4096', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '62.843'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.up_proj', 'loss': '0.15146972', 'samples': '4096', 'damp': '0.01000', 'time': '1.231', 'fwd_time': '68.441'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.gate_proj', 'loss': '0.18892969', 'samples': '4096', 'damp': '0.01000', 'time': '1.117', 'fwd_time': '68.441'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.down_proj', 'loss': '0.00983447', 'samples': '4096', 'damp': '0.01000', 'time': '3.921', 'fwd_time': '99.758'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '0.09149595', 'samples': '4096', 'damp': '0.01000', 'time': '1.512', 'fwd_time': '73.937'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '0.02792757', 'samples': '4096', 'damp': '0.01000', 'time': '1.310', 'fwd_time': '73.937'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '0.14742397', 'samples': '4096', 'damp': '0.01000', 'time': '1.460', 'fwd_time': '73.937'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.o_proj', 'loss': '0.00205535', 'samples': '4096', 'damp': '0.01000', 'time': '1.394', 'fwd_time': '63.090'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.up_proj', 'loss': '0.16773717', 'samples': '4096', 'damp': '0.01000', 'time': '1.165', 'fwd_time': '68.435'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.gate_proj', 'loss': '0.21768546', 'samples': '4096', 'damp': '0.01000', 'time': '1.219', 'fwd_time': '68.435'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.down_proj', 'loss': '0.01152225', 'samples': '4096', 'damp': '0.01000', 'time': '3.544', 'fwd_time': '99.899'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.k_proj', 'loss': '0.10135132', 'samples': '4096', 'damp': '0.01000', 'time': '1.443', 'fwd_time': '73.872'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.v_proj', 'loss': '0.04113655', 'samples': '4096', 'damp': '0.01000', 'time': '1.314', 'fwd_time': '73.872'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.q_proj', 'loss': '0.16089720', 'samples': '4096', 'damp': '0.01000', 'time': '1.275', 'fwd_time': '73.872'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.o_proj', 'loss': '0.00307318', 'samples': '4096', 'damp': '0.01000', 'time': '1.362', 'fwd_time': '63.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.up_proj', 'loss': '0.18329039', 'samples': '4096', 'damp': '0.01000', 'time': '1.213', 'fwd_time': '68.198'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.gate_proj', 'loss': '0.24332288', 'samples': '4096', 'damp': '0.01000', 'time': '1.322', 'fwd_time': '68.198'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 24, 'module': 'mlp.down_proj', 'loss': '0.01328651', 'samples': '4096', 'damp': '0.01000', 'time': '3.595', 'fwd_time': '99.900'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.k_proj', 'loss': '0.08178437', 'samples': '4096', 'damp': '0.01000', 'time': '1.191', 'fwd_time': '73.759'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.v_proj', 'loss': '0.03918345', 'samples': '4096', 'damp': '0.01000', 'time': '1.169', 'fwd_time': '73.759'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.q_proj', 'loss': '0.16061834', 'samples': '4096', 'damp': '0.01000', 'time': '1.405', 'fwd_time': '73.759'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.o_proj', 'loss': '0.00337070', 'samples': '4096', 'damp': '0.01000', 'time': '1.154', 'fwd_time': '62.867'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.up_proj', 'loss': '0.20346707', 'samples': '4096', 'damp': '0.01000', 'time': '1.263', 'fwd_time': '68.212'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.gate_proj', 'loss': '0.26730213', 'samples': '4096', 'damp': '0.01000', 'time': '1.344', 'fwd_time': '68.212'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 25, 'module': 'mlp.down_proj', 'loss': '0.01755981', 'samples': '4096', 'damp': '0.01000', 'time': '3.770', 'fwd_time': '99.051'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.k_proj', 'loss': '0.08656490', 'samples': '4096', 'damp': '0.01000', 'time': '1.352', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.v_proj', 'loss': '0.04972475', 'samples': '4096', 'damp': '0.01000', 'time': '1.370', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.q_proj', 'loss': '0.14920008', 'samples': '4096', 'damp': '0.01000', 'time': '1.260', 'fwd_time': '74.069'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.o_proj', 'loss': '0.00657041', 'samples': '4096', 'damp': '0.01000', 'time': '1.087', 'fwd_time': '63.091'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.up_proj', 'loss': '0.21160695', 'samples': '4096', 'damp': '0.01000', 'time': '1.309', 'fwd_time': '68.432'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.gate_proj', 'loss': '0.28265557', 'samples': '4096', 'damp': '0.01000', 'time': '1.215', 'fwd_time': '68.432'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 26, 'module': 'mlp.down_proj', 'loss': '0.02299990', 'samples': '4096', 'damp': '0.01000', 'time': '3.563', 'fwd_time': '99.554'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.k_proj', 'loss': '0.06566277', 'samples': '4096', 'damp': '0.01000', 'time': '1.472', 'fwd_time': '74.096'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.v_proj', 'loss': '0.03486685', 'samples': '4096', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '74.096'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.q_proj', 'loss': '0.12184440', 'samples': '4096', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '74.096'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.o_proj', 'loss': '0.01116214', 'samples': '4096', 'damp': '0.01000', 'time': '1.327', 'fwd_time': '63.057'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.up_proj', 'loss': '0.22326216', 'samples': '4096', 'damp': '0.01000', 'time': '1.179', 'fwd_time': '68.472'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.gate_proj', 'loss': '0.26846239', 'samples': '4096', 'damp': '0.01000', 'time': '1.200', 'fwd_time': '68.472'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 27, 'module': 'mlp.down_proj', 'loss': '0.00556553', 'samples': '4096', 'damp': '0.01250', 'time': '4.532', 'fwd_time': '100.122'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                                             \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`                       \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`                          \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TritonV2QuantLinear, TorchQuantLinear]`                              \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TritonV2QuantLinear`.                                                   \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                                                \n",
      "[6] Saving quantized model to: /kaggle/working/Llama3-3B-GPTQModel-4bit\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                                             \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 4,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "quantize_config.json\n",
      "generation_config.json\n",
      "quant_log.csv\n",
      "config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"do_sample\": true,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"temperature\": 0.6,\n",
      "    \"top_p\": 0.9,\n",
      "    \"transformers_version\": \"4.52.4\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 128000,\n",
      "    \"eos_token_id\": [\n",
      "        128001,\n",
      "        128008,\n",
      "        128009\n",
      "    ],\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 3072,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"max_position_embeddings\": 131072,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 24,\n",
      "    \"num_hidden_layers\": 28,\n",
      "    \"num_key_value_heads\": 8,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 4,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": {\n",
      "        \"factor\": 32.0,\n",
      "        \"high_freq_factor\": 4.0,\n",
      "        \"low_freq_factor\": 1.0,\n",
      "        \"original_max_position_embeddings\": 8192,\n",
      "        \"rope_type\": \"llama3\"\n",
      "    },\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"transformers_version\": \"4.52.4\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 128256\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 6127.86MB, 5.98GB                                                  \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 2151.27MB, 2.10GB                                                      \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 3976.59MB, 3.88GB - 64.89%                                                  \n"
     ]
    }
   ],
   "source": [
    "# Origin to GPTQ\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "\n",
    "# [1] 模型參數與儲存路徑\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "quant_path = \"/kaggle/working/Llama3-3B-GPTQModel-4bit\"\n",
    "\n",
    "# [2] 準備校準資料集（改用 wikitext-2）\n",
    "print(\"[2] Loading calibration dataset (WikiText-2)...\")\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "calibration_dataset = [x[\"text\"] for x in raw_dataset if len(x[\"text\"].strip()) > 0][:4096]\n",
    "\n",
    "# [3] 建立量化配置\n",
    "quant_config = QuantizeConfig(bits=4, group_size=128)\n",
    "\n",
    "# [4] 載入模型（會自動下載並準備 tokenizer）\n",
    "print(\"[4] Loading model with GPTQModel...\")\n",
    "model = GPTQModel.load(model_id, quant_config)\n",
    "\n",
    "# [5] 執行量化（依 GPU 記憶體情況調整 batch_size）\n",
    "print(\"[5] Quantizing model...\")\n",
    "model.quantize(calibration_dataset, batch_size=1)\n",
    "\n",
    "# [6] 儲存模型\n",
    "print(\"[6] Saving quantized model to:\", quant_path)\n",
    "model.save(quant_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-01T08:54:48.576930Z",
     "iopub.status.busy": "2025-06-01T08:54:48.576533Z",
     "iopub.status.idle": "2025-06-01T08:54:52.556159Z",
     "shell.execute_reply": "2025-06-01T08:54:52.554737Z",
     "shell.execute_reply.started": "2025-06-01T08:54:48.576891Z"
    },
    "id": "Hz8B2Sv-oSq_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d6d103f6-4ea6-49b0-fb7a-d3c80561cf55",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/2373169676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgptqmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPTQModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPTQModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_best_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBACKEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_const\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_best_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMODEL_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPTQModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseGPTQModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/models/_const.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBACKEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrocm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIS_ROCM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHAS_CUDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAS_MPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAS_XPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/utils/torch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# reset dynamo cache on each model load since during ci loop model inference may exhuast cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Increase the dynamo cache size limit, default of 8 is too low\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;31m# Lazy modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_lazy_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchdynamo_logging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m from .bytecode_analysis import (\n\u001b[1;32m     32\u001b[0m     \u001b[0mget_indexof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresume_execution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTORCH_DYNAMO_RESUME_IN_PREFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP_SUPPORTED_MODULES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrap_if_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from .variables import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mFunctionalCallVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnumVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .ctx_manager import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mCatchWarningsCtxManagerVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builtin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueMutationNew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEventVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStreamVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m from .dicts import (\n\u001b[1;32m     55\u001b[0m     \u001b[0mConstDictVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/ctx_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalStateSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .functions import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mNestedUserFunctionVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mUserFunctionVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fully_shard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fsdp_param_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0m_fsdp_param_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_flat_param\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlatParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFlatParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from ._fully_shard import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mCPUOffloadPolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFSDPModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfully_shard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_flat_param.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m from torch.distributed.fsdp._common_utils import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0m_FSDPDeviceHandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0m_named_parameters_with_duplicates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_common_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mno_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m from .api import (\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mFullOptimStateDictConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mFullStateDictConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# result.py use gptq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gptqmodel import GPTQModel\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 清空 PyTorch CUDA 記憶體\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# 清空 Python 變數參考\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# model_path = \"/kaggle/working/Llama3-3B-GPTQModel-4bit\"\n",
    "model_path = \"/kaggle/working/Qlora-GPTQModel-4bit\"\n",
    "\n",
    "\n",
    "def evaluate_ppl(model, tokenizer, device=\"cuda:0\"):\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    test_enc = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    model.seqlen = 2048\n",
    "    test_enc = test_enc.input_ids.to(device)\n",
    "\n",
    "    nsamples = test_enc.numel() // model.seqlen\n",
    "    nlls = []\n",
    "    for i in tqdm(range(nsamples), desc=\"Evaluating...\"):\n",
    "        batch = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * model.seqlen\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "    return ppl.item()\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    max_new_tokens = 256\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    quant_path = model_path\n",
    "    model = GPTQModel.load(quant_path, prefer_engine=\"exllamav2\")\n",
    "    tokenizer = model.tokenizer\n",
    "    model.eval()\n",
    "\n",
    "    # Attempt to show the kernel used from kernel_info fallback\n",
    "    try:\n",
    "        kernel_info = getattr(getattr(model.model, \"kernel_info\", None), \"name\", \"unknown\")\n",
    "    except:\n",
    "        print(\"no kernel info\")\n",
    "        kernel_info = \"unknown\"\n",
    "    print(f\"[Info] Loaded model with quant kernel: {kernel_info}\")\n",
    "    print(f\"[Info] Model class: {type(model).__name__}\")\n",
    "\n",
    "    prompt = \"How to learn a new language?\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    tputs = []\n",
    "    time_record = []\n",
    "    for _ in tqdm(range(10), desc=\"Test Inference\"):\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "\n",
    "        generated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms = start.elapsed_time(end)\n",
    "        tput = max_new_tokens / (elapsed_ms / 1000)\n",
    "        time_record.append(elapsed_ms / 1000)\n",
    "        tputs.append(tput)\n",
    "\n",
    "    response = tokenizer.decode(generated[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    sorted_tputs = np.sort(tputs)[2:-2]\n",
    "    org_tput = np.mean(sorted_tputs)\n",
    "    print(f'Prompt: {prompt}\\nResponse: {response}\\n')\n",
    "    print(f'Time Record: {time_record}')\n",
    "    print(f'Throughput Record: {tputs} toks/s\\n')\n",
    "\n",
    "    print(f'Throughput: {org_tput} toks/s')\n",
    "    ppl = evaluate_ppl(model, tokenizer, device)\n",
    "    print(f\"Perplexity (PPL): {ppl}\")\n",
    "\n",
    "    import csv\n",
    "    rounded_tput = round(org_tput, 1)\n",
    "    ppl = round(ppl, 2)\n",
    "\n",
    "    with open(\"result.csv\", mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Id\", \"value\"])\n",
    "        writer.writerow([0, ppl])\n",
    "        writer.writerow([1, rounded_tput])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7536527,
     "sourceId": 11983049,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0141a02d79dc456790e29ae79410af8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3af040e849b34c428d13d82a14ce6e84",
       "IPY_MODEL_a764c7a15e624f1a889e9dee4294b308",
       "IPY_MODEL_818d1edd2cea4c85810b230bd8afa686"
      ],
      "layout": "IPY_MODEL_0611c37987934b439d3e00b15ddbb48d"
     }
    },
    "0611c37987934b439d3e00b15ddbb48d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e9a85f10252477fb6c668f486fe87e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "100b37542725460eab99d1d71c63b5de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_765b3effcbda4b3a89f1fccdf66d082d",
      "placeholder": "​",
      "style": "IPY_MODEL_6b9b0ff72ac44773b59e418b4a1ab304",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "1275e7ca31d643a7b5e21f6e45ddd8ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "138d8fdef75f47748a440b30f4462f10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1464a3f1c2234b10b0325be3cde71053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d3761e3102d484a9cb1544e40c0e7b0",
      "placeholder": "​",
      "style": "IPY_MODEL_21e5df46d0614f37a8991bc780821378",
      "value": " 54.5k/54.5k [00:00&lt;00:00, 4.04MB/s]"
     }
    },
    "15c9ee01e4f84eac960c0ba22433052b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "173a908967e04e748157a98a28839227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19518e9f79a24bd081ee827dce8dffa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3ff1d0a0f40447fbf241a1899e96116",
      "placeholder": "​",
      "style": "IPY_MODEL_449674ec3b5245b69abde137ab334b6c",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "1b456e4baae74877914590363d795724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43fb490f6f3c488e87f82679a04bbcb8",
      "placeholder": "​",
      "style": "IPY_MODEL_7e6b2b45320c4a988dd87d02eafb61b5",
      "value": " 733k/733k [00:00&lt;00:00, 6.29MB/s]"
     }
    },
    "21e5df46d0614f37a8991bc780821378": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21ff6c8dc2fc4e77badc42f616415e70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2332d534786c4832b9764e0cbb2c4040": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f994ffd01c1e4e6ba3281f306166a55a",
      "placeholder": "​",
      "style": "IPY_MODEL_9b07890e03a148478ba6e4136379237f",
      "value": "Generating train split: 100%"
     }
    },
    "2364abb8d3464eccb71417a6cd887ee3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23b0735986244ad3ace09d01bb09527d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90e6f91f057e4325bac641a4c04c65b4",
      "placeholder": "​",
      "style": "IPY_MODEL_682b1f34cd9746999c0aab6b739d8bf4",
      "value": " 3760/3760 [00:00&lt;00:00, 190931.78 examples/s]"
     }
    },
    "26f170c80b0a401cac102aa286c764aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31a048dcb3aa4e0fa8795d9ca18ca552",
      "max": 4358,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be58c9aea387495f8f36d1857a05cbb7",
      "value": 4358
     }
    },
    "270dea18d7844aa5b24685095dac128d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e09b85a428ad48f7b681cfcefc52f821",
      "max": 732610,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_15c9ee01e4f84eac960c0ba22433052b",
      "value": 732610
     }
    },
    "27b6851d952a4fe193412622b740e887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2846c0f33e00431dbd7e87af31da8f91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "290bf769a3bf4cc1ad9739b506e1b695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b7b7c2bfbe644a0ac33859d089c1cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b6851d952a4fe193412622b740e887",
      "placeholder": "​",
      "style": "IPY_MODEL_ca2c42e36a4f4098ad981c0c7c4df552",
      "value": "train-00000-of-00001.parquet: 100%"
     }
    },
    "2d3761e3102d484a9cb1544e40c0e7b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31a048dcb3aa4e0fa8795d9ca18ca552": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3585db0fd71c414fac64fd45e65957b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4094a9c2fb8541be81d66d3a45ef6870",
      "placeholder": "​",
      "style": "IPY_MODEL_290bf769a3bf4cc1ad9739b506e1b695",
      "value": "test-00000-of-00001.parquet: 100%"
     }
    },
    "3af040e849b34c428d13d82a14ce6e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9b38c5c2bd942868816176b9478618b",
      "placeholder": "​",
      "style": "IPY_MODEL_8093f8dc686a40d08fe8e533d279f5ab",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "3f51300e5602430080bb9560c395fed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67fa155ad19544a7acfad297c37faf6c",
      "placeholder": "​",
      "style": "IPY_MODEL_b4bf9704c1844367a2258ff4659b02da",
      "value": " 657k/657k [00:00&lt;00:00, 38.1MB/s]"
     }
    },
    "3f95878e25f64cc0938d8b03c14002a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_100b37542725460eab99d1d71c63b5de",
       "IPY_MODEL_f299e2151e4e45108e3591f9bcb6be46",
       "IPY_MODEL_585e6bdee21d4c29a6205679fdbb0a14"
      ],
      "layout": "IPY_MODEL_4fc80e2196164c0388005afad061dd75"
     }
    },
    "4085e2d1424f4c0d8420827744665cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4094a9c2fb8541be81d66d3a45ef6870": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "433f40199bc043bdb1448f39e8b9b54b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43fb490f6f3c488e87f82679a04bbcb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "449674ec3b5245b69abde137ab334b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44c975612e714ce987fafbbe3c7f2688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee4c49ea5c5845f385bd10ad305d6560",
      "placeholder": "​",
      "style": "IPY_MODEL_c25935aaa83a483eb1726db9d3ef612e",
      "value": " 9.09M/9.09M [00:00&lt;00:00, 29.2MB/s]"
     }
    },
    "4623327d5c884c33be9fc12c2e5b839d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46982cb1f7374a5488378a1e494fe353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2846c0f33e00431dbd7e87af31da8f91",
      "placeholder": "​",
      "style": "IPY_MODEL_a9e3ebc2530e4252b3e0336eeb9d085a",
      "value": "README.md: 100%"
     }
    },
    "47469b8bcdf645b5a39c2a785649b1db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "477aacb0fa074f3988c375e8ef5fad49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_539a9b6df5bd4806a6881ea61a4126ea",
       "IPY_MODEL_26f170c80b0a401cac102aa286c764aa",
       "IPY_MODEL_9a09db00f94142c29fd93bac265ffafb"
      ],
      "layout": "IPY_MODEL_a14a7fa3d09a42b196db949d874130cd"
     }
    },
    "4fc80e2196164c0388005afad061dd75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5195aac8505c4a3b9a5c93b8c7d5557a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "539a9b6df5bd4806a6881ea61a4126ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef0eb6622dd54d41be39615660b29e5e",
      "placeholder": "​",
      "style": "IPY_MODEL_b19cfc4707e14755a7311bb0928e227b",
      "value": "Generating test split: 100%"
     }
    },
    "555cd1ca0215456482373546d3880051": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "585e6bdee21d4c29a6205679fdbb0a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d335c6dd74224a79a41892d40bc5bfbe",
      "placeholder": "​",
      "style": "IPY_MODEL_9676fd43da0a4ed3bd6481b1afce908b",
      "value": " 296/296 [00:00&lt;00:00, 34.1kB/s]"
     }
    },
    "5b1707d017d242e9949c2935c6df7a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3585db0fd71c414fac64fd45e65957b5",
       "IPY_MODEL_270dea18d7844aa5b24685095dac128d",
       "IPY_MODEL_1b456e4baae74877914590363d795724"
      ],
      "layout": "IPY_MODEL_47469b8bcdf645b5a39c2a785649b1db"
     }
    },
    "5b542d76d65049c48f680287cc673eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d8c6c1d44b347548dd1c87744344825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65b24569d21247c68299a2312816eddf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67fa155ad19544a7acfad297c37faf6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "682b1f34cd9746999c0aab6b739d8bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b708577c7374eb79adb6172d87cc2f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b9b0ff72ac44773b59e418b4a1ab304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cb19afba9664405b642b55a3b2f70f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f88082c8b7643438c62a2750e1e541e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_555cd1ca0215456482373546d3880051",
      "placeholder": "​",
      "style": "IPY_MODEL_b8a2263409c04e0a89e79c4c45a4b640",
      "value": " 10.5k/10.5k [00:00&lt;00:00, 1.05MB/s]"
     }
    },
    "718c87180a6a40fe9d7d26c3e2658f67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79013fa73a804f77ad02c52325ed20fd",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdc5cb6acb8f458587f357cf410d4c66",
      "value": 2
     }
    },
    "765b3effcbda4b3a89f1fccdf66d082d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79013fa73a804f77ad02c52325ed20fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b56c2a130044d519fb0069adf4631a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7de12965cbe048749dc6e36bb19dbb49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e4401c368e24eb1b5728a6d80e9f5d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e6b2b45320c4a988dd87d02eafb61b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f8e3e88b3dc46deb229e4865970a01c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8093f8dc686a40d08fe8e533d279f5ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "818d1edd2cea4c85810b230bd8afa686": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b42f6fbc755c4b1b9ead57ab67ed2777",
      "placeholder": "​",
      "style": "IPY_MODEL_c92b284a785649b5b06cba375571d751",
      "value": " 2/2 [00:25&lt;00:00, 11.38s/it]"
     }
    },
    "81d053c5358f4c6999dd11358a43b61b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "826104cc9c404d80b71400976a73d9d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "850f4ca3c069464c93bc4820051025e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d462f1607e4d481587974797a45ff720",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b708577c7374eb79adb6172d87cc2f3",
      "value": 9085657
     }
    },
    "85964b240d3c4ce79795112d2de51bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "885c52398f9f45b19806eef976087847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65b24569d21247c68299a2312816eddf",
      "placeholder": "​",
      "style": "IPY_MODEL_7f8e3e88b3dc46deb229e4865970a01c",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8d292fe2f97b43dba4162c363d5abf05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90c6cdb6e801476eb75eb1171c79957d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90d6fba305be463689ef0ec0851c4754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_885c52398f9f45b19806eef976087847",
       "IPY_MODEL_718c87180a6a40fe9d7d26c3e2658f67",
       "IPY_MODEL_b83ef0305cee4e87ba4ab2215e2fa6a9"
      ],
      "layout": "IPY_MODEL_81d053c5358f4c6999dd11358a43b61b"
     }
    },
    "90e6f91f057e4325bac641a4c04c65b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9676fd43da0a4ed3bd6481b1afce908b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a09db00f94142c29fd93bac265ffafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21ff6c8dc2fc4e77badc42f616415e70",
      "placeholder": "​",
      "style": "IPY_MODEL_8d292fe2f97b43dba4162c363d5abf05",
      "value": " 4358/4358 [00:00&lt;00:00, 5146.88 examples/s]"
     }
    },
    "9b07890e03a148478ba6e4136379237f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a14a7fa3d09a42b196db949d874130cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a764c7a15e624f1a889e9dee4294b308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b542d76d65049c48f680287cc673eda",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b97aefc59541608dd81e0acbe5878d",
      "value": 2
     }
    },
    "a8a6f97b17d34a0498364cee11dfaea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a9e3ebc2530e4252b3e0336eeb9d085a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa9a3982b722474fa3a54ff6e1697b52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abb460449f264eb2b5eff5668a4658df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46982cb1f7374a5488378a1e494fe353",
       "IPY_MODEL_c2175a08271a4a42a02760bd7df4418a",
       "IPY_MODEL_6f88082c8b7643438c62a2750e1e541e"
      ],
      "layout": "IPY_MODEL_5195aac8505c4a3b9a5c93b8c7d5557a"
     }
    },
    "acd05765bf7d4f4db44a82b3499e693c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad8191fc28134729a314834126b13a5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e4401c368e24eb1b5728a6d80e9f5d0",
      "placeholder": "​",
      "style": "IPY_MODEL_ef4311bf255a499a9b5901a07ce2a2ba",
      "value": "tokenizer.json: 100%"
     }
    },
    "b19cfc4707e14755a7311bb0928e227b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b42f6fbc755c4b1b9ead57ab67ed2777": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4bf9704c1844367a2258ff4659b02da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4fdc53dba884822b548ab16420bf2ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c854066d4f5c4aa4b6077aa8be6a819a",
       "IPY_MODEL_f1285292c5114eae9da51727c1e7e83b",
       "IPY_MODEL_3f51300e5602430080bb9560c395fed7"
      ],
      "layout": "IPY_MODEL_f1b6b98c63a343aeab8f58bb38d18ed3"
     }
    },
    "b81939b9ede9430d950ebcba71037b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b83ef0305cee4e87ba4ab2215e2fa6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85964b240d3c4ce79795112d2de51bf1",
      "placeholder": "​",
      "style": "IPY_MODEL_ba577dcfa9724c29b5fc03d70d86f68a",
      "value": " 2/2 [00:00&lt;00:00,  1.39it/s]"
     }
    },
    "b8a2263409c04e0a89e79c4c45a4b640": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8ab2eda5ee74a6a938db3152ab9396c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba577dcfa9724c29b5fc03d70d86f68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba73feb090c34e0980d0018d6dcaddcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bac2296043814b7bb11d7890388e8a7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b56c2a130044d519fb0069adf4631a4",
      "placeholder": "​",
      "style": "IPY_MODEL_1275e7ca31d643a7b5e21f6e45ddd8ce",
      "value": "Generating validation split: 100%"
     }
    },
    "be58c9aea387495f8f36d1857a05cbb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c2175a08271a4a42a02760bd7df4418a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_173a908967e04e748157a98a28839227",
      "max": 10464,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec167797c65a432d93962726051c1060",
      "value": 10464
     }
    },
    "c25935aaa83a483eb1726db9d3ef612e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c260485a020c4ab08191b9999bff5453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f858c2356b0e460f812159eb0653e2bd",
      "placeholder": "​",
      "style": "IPY_MODEL_f461efd65145423885a849af8362054c",
      "value": " 36718/36718 [00:00&lt;00:00, 500033.94 examples/s]"
     }
    },
    "c2b97aefc59541608dd81e0acbe5878d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3ff1d0a0f40447fbf241a1899e96116": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c854066d4f5c4aa4b6077aa8be6a819a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acd05765bf7d4f4db44a82b3499e693c",
      "placeholder": "​",
      "style": "IPY_MODEL_b8ab2eda5ee74a6a938db3152ab9396c",
      "value": "validation-00000-of-00001.parquet: 100%"
     }
    },
    "c8fffb29d7ee4dba8881b5f361c94010": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb78d9794fcd4984971867664127d91f",
      "max": 3760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b81939b9ede9430d950ebcba71037b12",
      "value": 3760
     }
    },
    "c92b284a785649b5b06cba375571d751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca2c42e36a4f4098ad981c0c7c4df552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb78d9794fcd4984971867664127d91f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdc5cb6acb8f458587f357cf410d4c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cdd03a1ebca94cb7a86aadab871473c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2364abb8d3464eccb71417a6cd887ee3",
      "max": 6357543,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4623327d5c884c33be9fc12c2e5b839d",
      "value": 6357543
     }
    },
    "d2637d151e464bc2a116fb8ed784e74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2332d534786c4832b9764e0cbb2c4040",
       "IPY_MODEL_e6a5e9a37f9a4160a613bb9256272771",
       "IPY_MODEL_c260485a020c4ab08191b9999bff5453"
      ],
      "layout": "IPY_MODEL_ba73feb090c34e0980d0018d6dcaddcd"
     }
    },
    "d335c6dd74224a79a41892d40bc5bfbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d462f1607e4d481587974797a45ff720": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cd1d3385534f95aaf96aee4aa9495a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d76e0a196e7841b2b0fc98689cb1e0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa9a3982b722474fa3a54ff6e1697b52",
      "max": 54528,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dce72fadda9e48009e046a833ffb1403",
      "value": 54528
     }
    },
    "d89c0ca6480b4283be5235fc1ead7b22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bac2296043814b7bb11d7890388e8a7c",
       "IPY_MODEL_c8fffb29d7ee4dba8881b5f361c94010",
       "IPY_MODEL_23b0735986244ad3ace09d01bb09527d"
      ],
      "layout": "IPY_MODEL_90c6cdb6e801476eb75eb1171c79957d"
     }
    },
    "dc1f097040a3426b83ac3cdc69155678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b7b7c2bfbe644a0ac33859d089c1cd6",
       "IPY_MODEL_cdd03a1ebca94cb7a86aadab871473c2",
       "IPY_MODEL_e037bb7bf07047879ca5dd6537b81b86"
      ],
      "layout": "IPY_MODEL_6cb19afba9664405b642b55a3b2f70f3"
     }
    },
    "dce72fadda9e48009e046a833ffb1403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dedacb59ff6e4aaeb0f4816cbd012fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad8191fc28134729a314834126b13a5a",
       "IPY_MODEL_850f4ca3c069464c93bc4820051025e8",
       "IPY_MODEL_44c975612e714ce987fafbbe3c7f2688"
      ],
      "layout": "IPY_MODEL_7de12965cbe048749dc6e36bb19dbb49"
     }
    },
    "e037bb7bf07047879ca5dd6537b81b86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5cd1d3385534f95aaf96aee4aa9495a",
      "placeholder": "​",
      "style": "IPY_MODEL_0e9a85f10252477fb6c668f486fe87e5",
      "value": " 6.36M/6.36M [00:00&lt;00:00, 46.7MB/s]"
     }
    },
    "e09b85a428ad48f7b681cfcefc52f821": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6a5e9a37f9a4160a613bb9256272771": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_826104cc9c404d80b71400976a73d9d7",
      "max": 36718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4085e2d1424f4c0d8420827744665cef",
      "value": 36718
     }
    },
    "ec167797c65a432d93962726051c1060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee4c49ea5c5845f385bd10ad305d6560": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef0eb6622dd54d41be39615660b29e5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef4311bf255a499a9b5901a07ce2a2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1285292c5114eae9da51727c1e7e83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff3e956f1f044171b842f8a0ac91b776",
      "max": 657209,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8a6f97b17d34a0498364cee11dfaea8",
      "value": 657209
     }
    },
    "f1b6b98c63a343aeab8f58bb38d18ed3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f299e2151e4e45108e3591f9bcb6be46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_138d8fdef75f47748a440b30f4462f10",
      "max": 296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_433f40199bc043bdb1448f39e8b9b54b",
      "value": 296
     }
    },
    "f461efd65145423885a849af8362054c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f50e536917f941ea80b601140d4dea4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19518e9f79a24bd081ee827dce8dffa6",
       "IPY_MODEL_d76e0a196e7841b2b0fc98689cb1e0bb",
       "IPY_MODEL_1464a3f1c2234b10b0325be3cde71053"
      ],
      "layout": "IPY_MODEL_5d8c6c1d44b347548dd1c87744344825"
     }
    },
    "f858c2356b0e460f812159eb0653e2bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f994ffd01c1e4e6ba3281f306166a55a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9b38c5c2bd942868816176b9478618b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3e956f1f044171b842f8a0ac91b776": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
