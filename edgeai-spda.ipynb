{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T08:53:47.089834Z",
     "iopub.status.busy": "2025-06-01T08:53:47.089545Z",
     "iopub.status.idle": "2025-06-01T08:53:47.132014Z",
     "shell.execute_reply": "2025-06-01T08:53:47.131303Z",
     "shell.execute_reply.started": "2025-06-01T08:53:47.089811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")  # 用你的 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-01T08:53:49.822659Z",
     "iopub.status.busy": "2025-06-01T08:53:49.822360Z",
     "iopub.status.idle": "2025-06-01T08:58:35.605572Z",
     "shell.execute_reply": "2025-06-01T08:58:35.604753Z",
     "shell.execute_reply.started": "2025-06-01T08:53:49.822634Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 08:53:57.067284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748768037.090065     143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748768037.096896     143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc33c65e2f147c991e6b61f5e6b7de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1589f1bfb06447a7b3baf69360b89651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warm Up...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6da60aef164a4ca798f4487bb95302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How to learn a new language?\n",
      "Response:  5 steps to success\n",
      "Learning a new language can be a challenging but rewarding experience. Here are five steps to help you achieve success:\n",
      "\n",
      "**Step 1: Set Your Goals and Motivation**\n",
      "\n",
      "Before you start learning a new language, it's essential to define your goals and motivation. Why do you want to learn a new language? Is it for travel, work, or personal enrichment? Setting specific, achievable goals will help you stay motivated and focused throughout the learning process.\n",
      "\n",
      "* Identify your target language and level of proficiency (beginner, intermediate, advanced).\n",
      "* Set realistic goals, such as passing a language proficiency test or being able to hold conversations with native speakers.\n",
      "* Find a language learning buddy or join a language exchange group to stay motivated and accountable.\n",
      "\n",
      "**Step 2: Choose the Right Learning Resources**\n",
      "\n",
      "There are many effective ways to learn a new language, and the right resources can make a significant difference in your success. Here are some popular options:\n",
      "\n",
      "* **Language learning apps:** Duolingo, Babbel, and Rosetta Stone are popular apps that offer interactive lessons and exercises.\n",
      "* **Language courses:** Enroll in a language course at a local college or language school, or online courses like Coursera or edX.\n",
      "* **Language exchange websites\n",
      "\n",
      "Time Record: [9.18232421875, 9.114021484375, 9.176744140625, 9.128677734375, 9.1401103515625, 9.247796875, 9.2052333984375, 9.2142275390625, 9.178115234375, 9.27251953125]\n",
      "Throughput Record: [27.879651589437074, 28.088588603711788, 27.896604294185387, 28.043491888864143, 28.008414576333518, 27.68226891877964, 27.810267151233074, 27.783121147673185, 27.892436896106677, 27.608461663222773] toks/s\n",
      "\n",
      "Throughput: 27.878415942494815 toks/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289077 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2969a37a4c346999681c712be213cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (PPL): 11.0475492477417\n"
     ]
    }
   ],
   "source": [
    "# spda result.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#####################################################################\n",
    "# === SPEC NOTICE ===\n",
    "# Only \"load model\" and \"generate\" function selection can be modified.\n",
    "# DO NOT change PPL calculation, timing, or throughput logic.\n",
    "#####################################################################\n",
    "\n",
    "# === (Optional) Define your own custom generate function. ===\n",
    "# This is useful if you want full control over KV cache and generation steps.\n",
    "# You can modify this function to suit your needs.\n",
    "# By default, we use model.generate() for simplicity and general use.\n",
    "def generate(model, input_ids, past_key_values, max_new_tokens):\n",
    "    input_ids = input_ids.clone()\n",
    "    with torch.no_grad():\n",
    "        # Prefill\n",
    "        outputs = model.prefill_forward(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=None,\n",
    "            attention_mask=None,\n",
    "            cache_position=None,\n",
    "            logits_to_keep=1\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        next_token = torch.argmax(outputs.logits, dim=-1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Token-by-token Decoding\n",
    "        for _ in range(max_new_tokens):\n",
    "            pos = input_ids.shape[1]\n",
    "            cache_position = torch.arange(pos, pos + 1, device=input_ids.device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                next_token,\n",
    "                past_key_values=past_key_values,\n",
    "                position_ids=cache_position.unsqueeze(0),\n",
    "                cache_position=cache_position\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def evaluate_ppl(model, tokenizer, device=\"cuda:0\"):\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    \n",
    "    test_enc = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    model.seqlen = 2048\n",
    "    test_enc = test_enc.input_ids.to(device)\n",
    "    \n",
    "    nsamples = test_enc.numel() // model.seqlen\n",
    "    nlls = []  \n",
    "    for i in tqdm(range(nsamples), desc=\"Evaluating...\"):\n",
    "        batch = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * model.seqlen\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "    \n",
    "    return ppl.item()\n",
    "\n",
    "def main():\n",
    "    ############## Set Up ##############\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    max_new_tokens = 256    # Number of new tokens to generate\n",
    "    device = 'cuda:0'\n",
    "    \n",
    "    ### === TODO: Load your model (you may change this part) ===\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"   \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device,\n",
    "        attn_implementation=\"sdpa\"  # 啟用原生 Attention 加速, spda\n",
    "    )\n",
    "    print(f\"Loaded model: {model_name}\")\n",
    "    \n",
    "    #####################################\n",
    "\n",
    "    \n",
    "    model.eval() \n",
    "    # add torch.compile(model) for performance optimization\n",
    "    # torch.compile(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # === (Optional) Uncomment the following lines if using the custom generate() function. ===\n",
    "    # model.prefill_forward = model.forward\n",
    "\n",
    "\n",
    "    warmup_prompt = \"Explain what AI is.\"\n",
    "    inputs = tokenizer(warmup_prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # === (Optional) Set up StaticCache for manual KV cache management ===\n",
    "    # from transformers import StaticCache\n",
    "    # past_key_values = StaticCache(\n",
    "    #     config=model.config, \n",
    "    #     max_batch_size=1, \n",
    "    #     max_cache_len=max_new_tokens + 16, \n",
    "    #     device=model.device, \n",
    "    #     dtype=torch.float16\n",
    "    # )\n",
    "    ####################################################################\n",
    "    \n",
    "    for i in tqdm(range(5), desc=\"Warm Up...\"):\n",
    "        #  === Default: use model.generate() for end-to-end warm-up === \n",
    "        _ = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "                \n",
    "        # === (Optional) Use custom generate() if uncommented ===\n",
    "        # generated = generate(model, input_ids, past_key_values, max_new_tokens)\n",
    "        # past_key_values.reset()\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    prompt = \"How to learn a new language?\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    tputs = []\n",
    "    time_record = []\n",
    "    for _ in tqdm(range(10), desc=\"Test Inference\"):\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "\n",
    "        # === Default: Use model.generate() for end-to-end timing === \n",
    "        generated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        # === Optional: Use custom generate() if uncommented ===\n",
    "        # generated = generate(model, input_ids, past_key_values, max_new_tokens)\n",
    "        # past_key_values.reset()\n",
    "\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms = start.elapsed_time(end)\n",
    "        tput = max_new_tokens / (elapsed_ms / 1000)\n",
    "        time_record.append(elapsed_ms / 1000)\n",
    "        tputs.append(tput)\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    response = tokenizer.decode(generated[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    sorted_tputs = np.sort(tputs)[2:-2]\n",
    "    org_tput = np.mean(sorted_tputs)\n",
    "    print(f'Prompt: {prompt}\\nResponse: {response}\\n')\n",
    "    \n",
    "    print(f'Time Record: {time_record}')\n",
    "    print(f'Throughput Record: {tputs} toks/s\\n')\n",
    "\n",
    "    ### Your final throughput result ###\n",
    "    print(f'Throughput: {org_tput} toks/s')\n",
    "    ppl = evaluate_ppl(model, tokenizer, device)\n",
    "    print(f\"Perplexity (PPL): {ppl}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    import csv\n",
    "    rounded_tput = round(org_tput, 1)\n",
    "    ppl = round(ppl, 2)\n",
    "\n",
    "    with open(\"result.csv\", mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Id\", \"value\"])\n",
    "        writer.writerow([0, ppl])\n",
    "        writer.writerow([1, rounded_tput])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
