{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"03132634575b423ca1da94c4b6dbe483":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038e31c4412d460ea453085dbdc42855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2929bc50a904c96b62e66cb4e048740","IPY_MODEL_e7db96dda6584ebbb734c8b8ab2bf123","IPY_MODEL_4d7bee1cefbc4025948c17dad9836708"],"layout":"IPY_MODEL_dc71ebdf35354ccaa97b5d9e94c37cdc"}},"05212cf48b514730b2f67626b242c8fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"063f8c8ae46a49d68e7bf224eeaa380b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"078607ce0baa425c90238f5f92b69421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b1b59830370497daf32c9252cf50064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_932594c5c14e402d9cedaec281e94aef","max":113,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c60b4c45950448f6a3ae4f6817f83392","value":113}},"10110ec891354744a2d21ba4c661c99c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11296f81ef4a4da7bf70fa560e768792":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_117d41f8d8f343929dc87ef934348f9f","placeholder":"​","style":"IPY_MODEL_32f66ac4387e4d91a1a60ff6807a51a5","value":"100%"}},"117d41f8d8f343929dc87ef934348f9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ad535761b6f471cb253357dea200b52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d3fe58a7e1e4bee921fb9af5f92a182":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f3e5c6c7e9b420c971499420194db2c","placeholder":"​","style":"IPY_MODEL_8f418be5b346452681c0cf9b43b82eb5","value":" 5/5 [00:28&lt;00:00,  4.09s/it]"}},"1f5fce2beedb4c3b8fa48643edb36885":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0644ef0ee254cde8f6a84b3aaa0b93f","placeholder":"​","style":"IPY_MODEL_3f69d8fb25bc416b9bf24d35548c378b","value":"Warm Up...: 100%"}},"203cbd51901c4689b18f23c544deab83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f5fce2beedb4c3b8fa48643edb36885","IPY_MODEL_a7a26b5bf98141b4bf9450638e4c7d9d","IPY_MODEL_1d3fe58a7e1e4bee921fb9af5f92a182"],"layout":"IPY_MODEL_ee2405b605a7403cbe6dbd7e5cea75c7"}},"231f8366e9a54985872443ccd3e1efd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03132634575b423ca1da94c4b6dbe483","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3457516cf88e4386a985f89960ae77cb","value":49}},"32f66ac4387e4d91a1a60ff6807a51a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3457516cf88e4386a985f89960ae77cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b85994d52934168bc7f250563908082":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0bc49c604444a79f5637a6fef8f97a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f69d8fb25bc416b9bf24d35548c378b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41cadea41bdb4385baae9e8a4c9f3cad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f21f2c1f35e84fb080b16de167ee72d0","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5214e4b0fd484be2b442a30f8639cb1d","value":10}},"430f733890644aa7b8ebdebe516a69b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7675468e66c4c65bec33dfbea575cbe","placeholder":"​","style":"IPY_MODEL_83df4d4852984f9fa806ff6f727fa94c","value":"100%"}},"4d7bee1cefbc4025948c17dad9836708":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_702b6f0c8b5743f78ca71554904ce8bb","placeholder":"​","style":"IPY_MODEL_078607ce0baa425c90238f5f92b69421","value":" 625/625 [00:50&lt;00:00, 13.13it/s]"}},"5214e4b0fd484be2b442a30f8639cb1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52d57fb1aeba4990a8a9093a5183868e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b85b3c0bc6e4e9b97c257a9097b8421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bea1178f5b4c4828a4a9ec1fb344be8a","IPY_MODEL_41cadea41bdb4385baae9e8a4c9f3cad","IPY_MODEL_dab1da798b794ee1a4d579db8a5bc791"],"layout":"IPY_MODEL_ddc44bd787824ccea5f48868c50a98f0"}},"636bb2de0fc1489f8f65d7a1a7fa1152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11296f81ef4a4da7bf70fa560e768792","IPY_MODEL_231f8366e9a54985872443ccd3e1efd1","IPY_MODEL_9462f79b89d64e61989bc7cb928bd362"],"layout":"IPY_MODEL_b06c9b27a58b481db958e8afae289436"}},"68fdab016c0d42c58380957d7040f6b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"702b6f0c8b5743f78ca71554904ce8bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72e58936d88d4d5cb204dbd8888751dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820c1773a9794f58a37ccbad81dcbf6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83df4d4852984f9fa806ff6f727fa94c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86baddd4477d4d2fa940256493b0d617":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b6e343e907a4851abf0b48acc2e87d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f3e5c6c7e9b420c971499420194db2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f418be5b346452681c0cf9b43b82eb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"901174c3825e4df3a885588f8a067726":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932594c5c14e402d9cedaec281e94aef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9462f79b89d64e61989bc7cb928bd362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86baddd4477d4d2fa940256493b0d617","placeholder":"​","style":"IPY_MODEL_db34b65b467d4f6ab10437a75a864814","value":" 49/49 [00:00&lt;00:00, 178.90it/s]"}},"97978619758e4f32b4e2d4341c9b9237":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97e023bd82be4f20b14ee0dafd33a1f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9faf4b2c10fa4064b45ec195958efbe2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97e023bd82be4f20b14ee0dafd33a1f1","placeholder":"​","style":"IPY_MODEL_eade00ff8c5e462295fe936051ad5ac8","value":" 113/113 [00:03&lt;00:00, 31.51it/s]"}},"a2929bc50a904c96b62e66cb4e048740":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad3b50c2c04946769e0fcafa25b25ed7","placeholder":"​","style":"IPY_MODEL_063f8c8ae46a49d68e7bf224eeaa380b","value":"100%"}},"a4875f2d29e64f5790cb2bcb5c55e322":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deeca1f999624af487aa3b49db181737","IPY_MODEL_b242476478d741978cbcfb266c519b49","IPY_MODEL_d5859f08bd0e43d09023735b119617b2"],"layout":"IPY_MODEL_3f0bc49c604444a79f5637a6fef8f97a"}},"a7a26b5bf98141b4bf9450638e4c7d9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4e865dbde39492dab21590d1c7ac960","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d00003ef73bc467bae7457c2816fd231","value":5}},"ad3b50c2c04946769e0fcafa25b25ed7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b06c9b27a58b481db958e8afae289436":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b242476478d741978cbcfb266c519b49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf04a1a320cd4aaa88ac8af8d39d33f2","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10110ec891354744a2d21ba4c661c99c","value":0}},"b4e865dbde39492dab21590d1c7ac960":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7675468e66c4c65bec33dfbea575cbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bea1178f5b4c4828a4a9ec1fb344be8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_901174c3825e4df3a885588f8a067726","placeholder":"​","style":"IPY_MODEL_8b6e343e907a4851abf0b48acc2e87d2","value":"Test Inference: 100%"}},"bf04a1a320cd4aaa88ac8af8d39d33f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60b4c45950448f6a3ae4f6817f83392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6422f5c6dcf46eebdf02c984e3ef94d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_430f733890644aa7b8ebdebe516a69b6","IPY_MODEL_0b1b59830370497daf32c9252cf50064","IPY_MODEL_9faf4b2c10fa4064b45ec195958efbe2"],"layout":"IPY_MODEL_3b85994d52934168bc7f250563908082"}},"ca09d6af91b8407e9afad018818b4972":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00003ef73bc467bae7457c2816fd231":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0644ef0ee254cde8f6a84b3aaa0b93f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5859f08bd0e43d09023735b119617b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05212cf48b514730b2f67626b242c8fe","placeholder":"​","style":"IPY_MODEL_97978619758e4f32b4e2d4341c9b9237","value":" 0/5 [06:18&lt;?, ?it/s]"}},"dab1da798b794ee1a4d579db8a5bc791":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d57fb1aeba4990a8a9093a5183868e","placeholder":"​","style":"IPY_MODEL_820c1773a9794f58a37ccbad81dcbf6d","value":" 10/10 [00:28&lt;00:00,  2.84s/it]"}},"db34b65b467d4f6ab10437a75a864814":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc71ebdf35354ccaa97b5d9e94c37cdc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddc44bd787824ccea5f48868c50a98f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deeca1f999624af487aa3b49db181737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e58936d88d4d5cb204dbd8888751dc","placeholder":"​","style":"IPY_MODEL_68fdab016c0d42c58380957d7040f6b1","value":"Warm Up...:   0%"}},"e7db96dda6584ebbb734c8b8ab2bf123":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca09d6af91b8407e9afad018818b4972","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ad535761b6f471cb253357dea200b52","value":625}},"eade00ff8c5e462295fe936051ad5ac8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2405b605a7403cbe6dbd7e5cea75c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f21f2c1f35e84fb080b16de167ee72d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":419495,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341987,"modelId":363308}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part1: Quantize Llama-3.2-3B-Instruct","metadata":{"id":"2PAW45yCMAM9"}},{"cell_type":"code","source":"!pip3 install huggingface-hub[cli]\n!pip3 install transformers==4.50.3\n!pip3 install torch torchvision torchaudio\n!pip3 install timm==1.0.15\n!pip3 install datasets==3.5.0\n!pip3 install accelerate==1.6.0\n!pip3 install gemlite==0.4.4\n!pip3 install hqq==0.2.5\n!pip3 install triton==3.2.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwWC3xDBlqzE","outputId":"27daf44e-8511-407d-e529-e2010e9b9292","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import float16\nfrom tqdm.auto import tqdm\n\nfrom typing import Union, Callable\nfrom functools import partial\nimport json\nimport timm\nimport os\n\nfrom hqq.core.quantize import HQQLinear\nfrom hqq.core.utils import cleanup\n\nfrom hqq.models.base import (\n    forward_device_hooked,\n    get_all_children_from_model,\n    find_parent,\n    is_leaf_module,\n    BaseHQQModel,\n    BasePatch\n)\n\nfrom hqq.models.hf.base import BaseHQQHFModel\n\n_QUANT_LAYERS = [nn.Linear, HQQLinear]\n_IGNORE_LINEAR = ['lm_head']\n\ndef get_size_of_model(model):\n    size_in_bytes = 0\n    for _, module in model.named_modules():\n        if isinstance(module, HQQLinear):\n            # W_q / Scale / Zero / Bias\n            size_in_bytes += module.W_q.numel() * module.W_q.element_size()\n            size_in_bytes += module.meta['scale'].numel() * module.meta['scale'].element_size()\n            size_in_bytes += module.meta['zero'].numel() * module.meta['zero'].element_size()\n\n            if isinstance(getattr(module, 'bias'), torch.Tensor):\n                size_in_bytes += module.bias.numel() * module.bias.element_size()\n\n        elif is_leaf_module(module):\n            for param in module.parameters():\n                size_in_bytes += param.numel() * param.element_size()\n            for buffer in module.buffers():\n                size_in_bytes += buffer.numel() * buffer.element_size()\n\n    return size_in_bytes\n\n# Get all linear tags available\ndef get_linear_tags_from_model(model, ignore: list) -> list:\n    linear_tags = set()\n    for name, module in model.named_modules():\n        if (type(module) in _QUANT_LAYERS) and (name.split(\".\")[-1] not in ignore):\n            linear_tags.add(name)\n    return list(linear_tags)\n\nclass CustomPatch(BasePatch):\n    # This method iterates through layers of the model that are nn.Linear and processes them via new_nodule = patch_fct(module, params)\n    @classmethod\n    def patch_linearlayers(\n        cls,\n        model,\n        patch_fct: Callable,\n        patch_params: Union[dict, None],\n        verbose: bool = True,\n    ) -> None:\n        ignore_tags = cls.get_ignore_layers(model)\n\n        tmp_mapping = {}\n        for name, module in model.named_modules():\n            if (type(module) in _QUANT_LAYERS) and (name not in ignore_tags):\n                tmp_mapping[name] = module\n\n        for name in tqdm(tmp_mapping, disable=not verbose):\n            linear_tag = name\n            patch_param = (\n                patch_params[linear_tag] if (linear_tag in patch_params) else None\n            )\n            setattr(\n                find_parent(model, name),\n                name.split(\".\")[-1],\n                patch_fct(tmp_mapping[name], patch_param),\n            )\n\n        cleanup()\n\n        # These tags are used to specfiy parameters of the patching in patch_linearlayers()\n    @classmethod\n    def set_auto_linear_tags(cls, model, ignore: list = _IGNORE_LINEAR) -> None:\n        if hasattr(model, \"linear_tags\") is False:\n            linear_tags = cls.get_linear_tags()\n            model.linear_tags = (\n                linear_tags\n                if len(linear_tags) > 0\n                else get_linear_tags_from_model(model, ignore=ignore)\n            )\n            model.base_class = cls\n\nclass CustomHQQTimmModel(BaseHQQModel):\n    # Create empty model\n    @classmethod\n    def create_model(cls, save_dir, kwargs):\n        with open(cls.get_config_file(save_dir), \"r\") as file:\n            config = json.load(file)\n        model = timm.create_model(\n            config[\"architecture\"] + \".\" + config[\"tag\"], pretrained=False\n        )\n        return model\n\n    # Save model architecture\n    @classmethod\n    def cache_model(cls, model, save_dir):\n        try:\n            os.makedirs(save_dir, exist_ok=True)\n        except Exception as error:\n            print(error)\n\n        with open(cls.get_config_file(save_dir), \"w\") as file:\n            json.dump(model.default_cfg, file)\n\n    # Main function to quantize a model. Basically goes through the linear layers specfied in the patching function and replaces them with HQQLinear\n    @classmethod\n    def quantize_model(\n        cls,\n        model,\n        quant_config: dict,\n        compute_dtype: torch.dtype = float16,\n        device: Union[str, list, dict] = \"cuda\",\n    ):\n        # Check if the model was already quantized\n        if getattr(model, \"hqq_quantized\", False):\n            print(\"Model was already quantized\")\n            return\n\n        # Set linear tags automatically\n        cls.setup_model(model)\n\n        # Use the same quantization config for all linear layers. Use None to skip quantizing a specfic layer.\n        if True in [(key in model.linear_tags) for key in quant_config.keys()]:\n            # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n            patch_params = {key: None for key in model.linear_tags}\n            patch_params.update(quant_config)\n        elif quant_config == {}:\n            patch_params = {key: None for key in model.linear_tags}\n        else:\n            # Same quant_config for all layers\n            patch_params = {k: quant_config for k in model.linear_tags}\n\n        # Get list of all nodes in order\n        all_nodes = get_all_children_from_model(model, [])  # ordered nodes\n        try:\n            # Extract block names: This is following Hugging Face models.\n            num_blocks = (\n                len(model.model.blocks)   # TODO: Modify layers to blocks\n                if hasattr(model, \"model\")\n                else len(model.blocks)\n            )\n            all_blocks = [\"blocks.\" + str(i) for i in range(num_blocks)]\n        except Exception:\n            all_blocks = None\n            print(\n                \"Default model structure not supported. Make sure you feed device as dictionary as {name_block: device}\"\n            )\n\n        if isinstance(\n            device, dict\n        ):  # input as {module block name (str): device (str or torch.device)}\n            device_map = device\n            num_devices = len(set([device_map[k] for k in device_map]))\n            all_blocks = list(device_map.keys())\n\n        node_to_block = {}\n        for node in all_nodes:\n            res = [block for block in all_blocks if (block in node)]\n            node_to_block[node] = res[-1] if (len(res) > 0) else node\n\n        # Set device-map\n        if isinstance(device, str):  # single device as str\n            device_map = {k: device for k in all_blocks + all_nodes}\n            num_devices = 1\n\n        if isinstance(device, list):  # list of devices\n            num_devices = len(device)\n            device_map = {}\n            for node in all_nodes:\n                if \".blocks\" in node:\n                    break\n                device_map[node] = device[0]\n\n            for node in all_nodes[::-1]:\n                if \".blocks\" in node:\n                    break\n                device_map[node] = device[-1]\n\n            step, k = len(all_blocks) // num_devices, 0\n            for i in range(0, len(all_blocks), step):\n                for j in range(i, i + step):\n                    device_map[all_blocks[min(j, len(all_blocks) - 1)]] = device[\n                        min(k, num_devices - 1)\n                    ]\n                k += 1\n\n        # Map nodes to block devices\n        for node in all_nodes:\n            device_map[node] = device_map[node_to_block[node]]\n\n        # print(device_map)\n\n        # We replace the nn.Linear layers with HQQLinear\n        def _patch_linear(linear_layer, quant_config):\n            if type(linear_layer) is HQQLinear:\n                return linear_layer\n\n            current_device = device_map[linear_layer.name]\n            if quant_config is not None:\n                out_module = HQQLinear(\n                    linear_layer,\n                    quant_config,\n                    compute_dtype=compute_dtype,\n                    device=current_device,\n                )\n            else:\n                out_module = linear_layer.to(device=current_device, dtype=compute_dtype)\n\n            out_module.device = current_device\n            return out_module\n\n        def _patch_other(layer):\n            current_device = device_map[layer.name]\n            layer.device = current_device\n            return layer.to(device=current_device, dtype=compute_dtype)\n\n        cls.patch_model(model, _patch_other, _patch_linear, patch_params)\n\n        # Insert device switcher\n        if num_devices > 1:\n            core_model = model if hasattr(model, \"blocks\") else model.model\n\n            # Make sure the input (first node) has the input in the right device during generation\n            input_node_child_name = all_nodes[0].split(\".\")[-1]\n            input_node = getattr(core_model, input_node_child_name)\n            input_node.device = device_map[all_nodes[0]]\n            input_node.forward_orig = input_node.forward\n            input_node.forward = partial(forward_device_hooked, input_node)\n            setattr(core_model, input_node_child_name, input_node)\n\n            # Make sure all inputs to the blocks are in the right device\n            for i in range(len(core_model.blocks)):\n                core_model.blocks[i].device = device_map[core_model.blocks[i].name]\n                core_model.blocks[i].forward_orig = core_model.blocks[i].forward\n                core_model.blocks[i].forward = partial(\n                    forward_device_hooked, core_model.blocks[i]\n                )\n\n        # Set base class\n        model.base_class = cls\n\n        model.hqq_quantized = True\n\n        return model\n\nclass CustomHQQHFModel(BaseHQQHFModel):\n    # Main function to quantize a model. Basically goes through the linear layers specfied in the patching function and replaces them with HQQLinear\n    @classmethod\n    def quantize_model(\n        cls,\n        model,\n        quant_config: dict,\n        compute_dtype: torch.dtype = float16,\n        device: Union[str, list, dict] = \"cuda\",\n    ):\n        # Check if the model was already quantized\n        if getattr(model, \"hqq_quantized\", False):\n            print(\"Model was already quantized\")\n            return\n\n        # Set linear tags automatically\n        cls.setup_model(model)\n\n        # Use the same quantization config for all linear layers. Use None to skip quantizing a specfic layer.\n        if True in [(key in model.linear_tags) for key in quant_config.keys()]:\n            # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n            patch_params = {key: None for key in model.linear_tags}\n            patch_params.update(quant_config)\n        elif quant_config == {}:\n            patch_params = {key: None for key in model.linear_tags}\n        else:\n            # Same quant_config for all layers\n            patch_params = {k: quant_config for k in model.linear_tags}\n\n        # Get list of all nodes in order\n        all_nodes = get_all_children_from_model(model, [])  # ordered nodes\n        try:\n            # Extract block names: This is following Hugging Face models.\n            num_blocks = (\n                len(model.model.layers)\n                if hasattr(model, \"model\")\n                else len(model.layers)\n            )\n            all_blocks = [\"model.layers.\" + str(i) for i in range(num_blocks)]\n        except Exception:\n            all_blocks = None\n            print(\n                \"Default model structure not supported. Make sure you feed device as dictionary as {name_block: device}\"\n            )\n\n        if isinstance(\n            device, dict\n        ):  # input as {module block name (str): device (str or torch.device)}\n            device_map = device\n            num_devices = len(set([device_map[k] for k in device_map]))\n            all_blocks = list(device_map.keys())\n\n        node_to_block = {}\n        for node in all_nodes:\n            res = [block for block in all_blocks if (block in node)]\n            node_to_block[node] = res[-1] if (len(res) > 0) else node\n\n        # Set device-map\n        if isinstance(device, str):  # single device as str\n            device_map = {k: device for k in all_blocks + all_nodes}\n            num_devices = 1\n\n        if isinstance(device, list):  # list of devices\n            num_devices = len(device)\n            device_map = {}\n            for node in all_nodes:\n                if \".layers\" in node:\n                    break\n                device_map[node] = device[0]\n\n            for node in all_nodes[::-1]:\n                if \".layers\" in node:\n                    break\n                device_map[node] = device[-1]\n\n            step, k = len(all_blocks) // num_devices, 0\n            for i in range(0, len(all_blocks), step):\n                for j in range(i, i + step):\n                    device_map[all_blocks[min(j, len(all_blocks) - 1)]] = device[\n                        min(k, num_devices - 1)\n                    ]\n                k += 1\n\n        # Map nodes to block devices\n        for node in all_nodes:\n            device_map[node] = device_map[node_to_block[node]]\n\n        # print(device_map)\n\n        # We replace the nn.Linear layers with HQQLinear\n        def _patch_linear(linear_layer, quant_config):\n            if type(linear_layer) is HQQLinear:\n                return linear_layer\n\n            current_device = device_map[linear_layer.name]\n            if quant_config is not None:\n                out_module = HQQLinear(\n                    linear_layer,\n                    quant_config,\n                    compute_dtype=compute_dtype,\n                    device=current_device,\n                )\n            else:\n                out_module = linear_layer.to(device=current_device, dtype=compute_dtype)\n\n            out_module.device = current_device\n            return out_module\n\n        def _patch_other(layer):\n            current_device = device_map[layer.name]\n            layer.device = current_device\n            return layer.to(device=current_device, dtype=compute_dtype)\n\n        cls.patch_model(model, _patch_other, _patch_linear, patch_params)\n\n        # Insert device switcher\n        if num_devices > 1:\n            core_model = model if hasattr(model, \"layers\") else model.model\n\n            # Make sure the input (first node) has the input in the right device during generation\n            input_node_child_name = all_nodes[0].split(\".\")[-1]\n            input_node = getattr(core_model, input_node_child_name)\n            input_node.device = device_map[all_nodes[0]]\n            input_node.forward_orig = input_node.forward\n            input_node.forward = partial(forward_device_hooked, input_node)\n            setattr(core_model, input_node_child_name, input_node)\n\n            # Make sure all inputs to the blocks are in the right device\n            for i in range(len(core_model.layers)):\n                core_model.layers[i].device = device_map[core_model.layers[i].name]\n                core_model.layers[i].forward_orig = core_model.layers[i].forward\n                core_model.layers[i].forward = partial(\n                    forward_device_hooked, core_model.layers[i]\n                )\n\n        # Set base class\n        model.base_class = cls\n\n        model.hqq_quantized = True\n\n        return model\n\n# Auto class used for HF models if no architecture was manually setup\nclass AutoHQQHFModel(CustomHQQHFModel, CustomPatch):\n    pass\n\nclass AutoHQQTimmModel(CustomHQQTimmModel, CustomPatch):\n    pass","metadata":{"id":"U1WJOPqHlfiA","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:11:45.367753Z","iopub.execute_input":"2025-06-01T08:11:45.368445Z","iopub.status.idle":"2025-06-01T08:11:46.705248Z","shell.execute_reply.started":"2025-06-01T08:11:45.368413Z","shell.execute_reply":"2025-06-01T08:11:46.704673Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nfrom tqdm.auto import tqdm\n\nfrom torchvision import datasets, transforms\nfrom timm.data import create_transform\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom torch.utils.data import DataLoader\n\ndef build_transform(is_train):\n    input_size = 224\n    eval_crop_ratio = 1.0\n\n    resize_im = input_size > 32\n    if is_train:\n        # this should always dispatch to transforms_imagenet_train\n        transform = create_transform(\n            input_size=input_size,\n            is_training=True,\n            color_jitter=0.3,\n            auto_augment='rand-m9-mstd0.5-inc1',\n            interpolation='bicubic',\n            re_prob=0.0,\n            re_mode='pixel',\n            re_count=1,\n        )\n        if not resize_im:\n            # replace RandomResizedCropAndInterpolation with\n            # RandomCrop\n            transform.transforms[0] = transforms.RandomCrop(\n                input_size, padding=4)\n        return transform\n\n    t = []\n    if resize_im:\n        size = int(input_size / eval_crop_ratio)\n        t.append(\n            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n        )\n        t.append(transforms.CenterCrop(input_size))\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n    return transforms.Compose(t)\n\ndef build_dataset_CIFAR100(is_train, data_path):\n    transform = build_transform(is_train)\n    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n    nb_classes = 100\n    return dataset, nb_classes\n\ndef prepare_data(batch_size):\n    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n    return train_loader, test_loader, nb_classes\n\ndef evaluate_model(model, data_loader, device):\n    model.to(device)\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    # print(f'Accuracy of the model on the test images: {accuracy}%')\n    return accuracy\n","metadata":{"id":"PoUDSIPzoOu9","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:11:56.045714Z","iopub.execute_input":"2025-06-01T08:11:56.046016Z","iopub.status.idle":"2025-06-01T08:11:56.055892Z","shell.execute_reply.started":"2025-06-01T08:11:56.045991Z","shell.execute_reply":"2025-06-01T08:11:56.054888Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from hqq.core.quantize import BaseQuantizeConfig\nfrom huggingface_hub import login\nlogin(\"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\")","metadata":{"id":"92ottf6VrLBu","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:11:59.807565Z","iopub.execute_input":"2025-06-01T08:11:59.807858Z","iopub.status.idle":"2025-06-01T08:11:59.929521Z","shell.execute_reply.started":"2025-06-01T08:11:59.807834Z","shell.execute_reply":"2025-06-01T08:11:59.928952Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:02.590008Z","iopub.execute_input":"2025-06-01T08:12:02.590342Z","iopub.status.idle":"2025-06-01T08:12:02.819800Z","shell.execute_reply.started":"2025-06-01T08:12:02.590318Z","shell.execute_reply":"2025-06-01T08:12:02.818869Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Throughput: 60.09128305820182 toks/s\n# Perplexity (PPL): 11.403003692626953\ndef get_quant_config_slm(model):\n    quant_config = {}\n\n    n_layers = model.config.num_hidden_layers\n\n    # 定義不同精度等級的量化參數\n    q_very_light = BaseQuantizeConfig(nbits=4, group_size=32)\n    q_base       = BaseQuantizeConfig(nbits=4, group_size=64)\n    q_down_light = BaseQuantizeConfig(nbits=4, group_size=128)  # 專供 down_proj 用\n    q_important  = BaseQuantizeConfig(nbits=8, group_size=64)\n    q_important_middle = BaseQuantizeConfig(nbits=8, group_size=128)\n    q_important_light = BaseQuantizeConfig(nbits=8, group_size=256)\n\n    for i in range(n_layers):\n\n        quant_config[f'model.layers.{i}.self_attn.q_proj'] = q_very_light\n\n        quant_config[f'model.layers.{i}.self_attn.k_proj'] = q_important_light\n        quant_config[f'model.layers.{i}.self_attn.v_proj'] = q_important_light\n\n        quant_config[f'model.layers.{i}.self_attn.o_proj'] = q_important_middle\n\n        quant_config[f'model.layers.{i}.mlp.gate_proj'] = q_down_light\n        quant_config[f'model.layers.{i}.mlp.up_proj'] = q_down_light\n\n        quant_config[f'model.layers.{i}.mlp.down_proj'] = q_down_light\n\n    return quant_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:04.299983Z","iopub.execute_input":"2025-06-01T08:12:04.300313Z","iopub.status.idle":"2025-06-01T08:12:04.306338Z","shell.execute_reply.started":"2025-06-01T08:12:04.300288Z","shell.execute_reply":"2025-06-01T08:12:04.305599Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nimport random\nimport numpy as np\n\nfrom hqq.utils.patching import recommended_inductor_config_setter\n\ndef generate(model, input_ids, past_key_values, max_new_tokens, activate_timing, verbose=True):\n    input_ids = input_ids.clone()\n    tput = None\n    # Run an initial forward pass to compute and store the static KV cache\n    if verbose:\n        print('Prefilling...')\n    with torch.no_grad():\n        # outputs = custom_forward(model, input_ids, past_key_values=past_key_values, use_cache=True, position_ids=None, attention_mask=None, cache_position=None, is_compiled=False)\n        outputs = model.prefill_forward(input_ids, past_key_values=past_key_values, position_ids=None, attention_mask=None, cache_position=None, logits_to_keep=1)\n        past_key_values = outputs.past_key_values\n        next_token = torch.argmax(outputs.logits, dim=-1)\n        input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n    # Generate tokens one by one using a for loop and update the KV cache\n    if verbose:\n        print('Decoding...')\n    with torch.no_grad():\n        if activate_timing:\n            start_event = torch.cuda.Event(enable_timing=True)\n            end_event = torch.cuda.Event(enable_timing=True)\n            start_event.record()\n        for _ in range(max_new_tokens):\n            # Compute position_ids using the current sequence length\n            pos = input_ids.shape[1]\n            cache_position = torch.arange(pos, pos+1, device=input_ids.device, dtype=torch.long)\n\n            # Run the model on the last token using the cached key-value pairs\n            outputs = model(\n                next_token,\n                past_key_values=past_key_values,\n                position_ids=cache_position.unsqueeze(0),\n                cache_position=cache_position\n            )\n            logits = outputs.logits\n\n            # Greedily select the token with the highest probability\n            next_token = torch.argmax(logits, dim=-1)\n\n            # Append the predicted token to the generated sequence\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n            # Update the KV cache for the next iteration\n            past_key_values = outputs.past_key_values\n        if activate_timing:\n            end_event.record()\n        torch.cuda.synchronize()\n    if activate_timing:\n        tput = max_new_tokens / start_event.elapsed_time(end_event) * 1000\n        # print(f\"Throughput: {tput} toks/sec\")\n    return input_ids, tput\n\ndef evaluate_ppl(model, tokenizer, device=\"cuda:0\"):\n    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    # print(f\"Dataset length: {len(test_dataset)}\")\n\n    test_enc = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    model.seqlen = 2048\n    test_enc = test_enc.input_ids.to(device)\n\n    nsamples = test_enc.numel() // model.seqlen\n    nlls = []\n    for i in tqdm(range(nsamples), desc=\"Evaluating...\"):\n        batch = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)]\n\n        with torch.no_grad():\n            lm_logits = model(batch).logits\n\n        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n        shift_labels = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * model.seqlen\n        nlls.append(neg_log_likelihood)\n\n    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n\n    return ppl.item()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764,"referenced_widgets":["203cbd51901c4689b18f23c544deab83","1f5fce2beedb4c3b8fa48643edb36885","a7a26b5bf98141b4bf9450638e4c7d9d","1d3fe58a7e1e4bee921fb9af5f92a182","ee2405b605a7403cbe6dbd7e5cea75c7","d0644ef0ee254cde8f6a84b3aaa0b93f","3f69d8fb25bc416b9bf24d35548c378b","b4e865dbde39492dab21590d1c7ac960","d00003ef73bc467bae7457c2816fd231","8f3e5c6c7e9b420c971499420194db2c","8f418be5b346452681c0cf9b43b82eb5","5b85b3c0bc6e4e9b97c257a9097b8421","bea1178f5b4c4828a4a9ec1fb344be8a","41cadea41bdb4385baae9e8a4c9f3cad","dab1da798b794ee1a4d579db8a5bc791","ddc44bd787824ccea5f48868c50a98f0","901174c3825e4df3a885588f8a067726","8b6e343e907a4851abf0b48acc2e87d2","f21f2c1f35e84fb080b16de167ee72d0","5214e4b0fd484be2b442a30f8639cb1d","52d57fb1aeba4990a8a9093a5183868e","820c1773a9794f58a37ccbad81dcbf6d","c6422f5c6dcf46eebdf02c984e3ef94d","430f733890644aa7b8ebdebe516a69b6","0b1b59830370497daf32c9252cf50064","9faf4b2c10fa4064b45ec195958efbe2","3b85994d52934168bc7f250563908082","b7675468e66c4c65bec33dfbea575cbe","83df4d4852984f9fa806ff6f727fa94c","932594c5c14e402d9cedaec281e94aef","c60b4c45950448f6a3ae4f6817f83392","97e023bd82be4f20b14ee0dafd33a1f1","eade00ff8c5e462295fe936051ad5ac8","a4875f2d29e64f5790cb2bcb5c55e322","deeca1f999624af487aa3b49db181737","b242476478d741978cbcfb266c519b49","d5859f08bd0e43d09023735b119617b2","3f0bc49c604444a79f5637a6fef8f97a","72e58936d88d4d5cb204dbd8888751dc","68fdab016c0d42c58380957d7040f6b1","bf04a1a320cd4aaa88ac8af8d39d33f2","10110ec891354744a2d21ba4c661c99c","05212cf48b514730b2f67626b242c8fe","97978619758e4f32b4e2d4341c9b9237"]},"id":"5b9yYg18pR2k","outputId":"47e63067-b371-4704-e4e3-d26ee77d8dc1","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:06.230284Z","iopub.execute_input":"2025-06-01T08:12:06.230581Z","iopub.status.idle":"2025-06-01T08:12:07.775560Z","shell.execute_reply.started":"2025-06-01T08:12:06.230559Z","shell.execute_reply":"2025-06-01T08:12:07.774933Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"torch._dynamo.reset()\n############## Set Up ##############\ntorch.manual_seed(0)\nrandom.seed(0)\nrecommended_inductor_config_setter()\n\nmax_new_tokens = 256    # Number of new tokens to generate\ndevice = 'cuda:0'\nbackend = 'gemlite'\n\nmodel_name = \"meta-llama/Llama-3.2-3B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=device,\n    token = \"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\"\n)\ntorch.compile(model)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Separate Prefill & Decode Forwarding Function\nmodel.prefill_forward = model.forward\nmodel.forward = torch.compile(model.forward, mode='max-autotune', dynamic=False, fullgraph=True)\n\nprint(f'Model Size Before Quant: {get_size_of_model(model) / (1024 ** 2)} MiB')\n\n# TODO: Quantize\nquant_config = get_quant_config_slm(model)\n\nAutoHQQHFModel.quantize_model(model, quant_config=quant_config, compute_dtype=torch.float16, device=device)\n\nsave_dir = \"/kaggle/working/hqq_Llama3.2-3B-Instruct\"\nAutoHQQHFModel.save_quantized(model, save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:09.704175Z","iopub.execute_input":"2025-06-01T08:12:09.704749Z","iopub.status.idle":"2025-06-01T08:12:50.164182Z","shell.execute_reply.started":"2025-06-01T08:12:09.704725Z","shell.execute_reply":"2025-06-01T08:12:50.162811Z"}},"outputs":[{"name":"stderr","text":"2025-06-01 08:12:10.272633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748765530.297582   11201 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748765530.305148   11201 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c886f75c5a694d7e82f7aa9de301eb69"}},"metadata":{}},{"name":"stdout","text":"Model Size Before Quant: 6879.334228515625 MiB\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 87/87 [00:00<00:00, 34986.04it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/197 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6cdbb0875b4ac09ffb99841654fef8"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Optimized Inference/Prediction/Eval","metadata":{}},{"cell_type":"code","source":"from hqq.utils.patching import prepare_for_inference\nprepare_for_inference(model, backend=backend)\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:53.345466Z","iopub.execute_input":"2025-06-01T08:12:53.346459Z","iopub.status.idle":"2025-06-01T08:12:56.505138Z","shell.execute_reply.started":"2025-06-01T08:12:53.346433Z","shell.execute_reply":"2025-06-01T08:12:56.504270Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"warmup_prompt = \"Explain what AI is.\"\ninput_ids = tokenizer(warmup_prompt, return_tensors=\"pt\").input_ids.to(device)\npast_key_values = StaticCache(\n    config=model.config,\n    max_batch_size=1,\n    max_cache_len=max_new_tokens + 16,\n    device=model.device,\n    dtype=torch.float16\n)\nfor i in tqdm(range(5), desc=\"Warm Up...\"):\n    generated = generate(model, input_ids, past_key_values, max_new_tokens, activate_timing=False, verbose=False)\n    past_key_values.reset()\n\nprompt = \"How to learn a new language?\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n\nfor i in range(20):\n    tputs = []\n    for _ in tqdm(range(10), desc=\"Test Inference\"):\n        generated, tput = generate(model, input_ids, past_key_values, max_new_tokens, activate_timing=True, verbose=False)\n        past_key_values.reset()\n        tputs.append(tput)\n    response = tokenizer.decode(generated[0][input_ids.shape[1]:], skip_special_tokens=True)\n    tputs = np.sort(tputs)[2:-2]\n    quant_tput = np.mean(tputs)\n    print(f'Prompt: {prompt}\\nResponse: {response}\\nThroughput: {quant_tput} toks/s')\n    if quant_tput >= 62:\n        break\n\nprint(f'Model Size After Quant: {get_size_of_model(model) / (1024 ** 2)} MiB')\n\nppl = evaluate_ppl(model, tokenizer, device)\nprint(f\"Perplexity (PPL): {ppl}\")\n# print(f\"Speedup: {quant_tput / org_tput} x\")\n\nscore = 0\nscore += 10 if quant_tput >= 31.0 else 0\nscore += 30 if quant_tput >= 54.0 else 0\nif ppl > 11.5:\n    score = 0 \nprint(f'Score: {score}')\n\n# torch.save(model.state_dict(), \"/kaggle/working/llama3_quantized.pth\")\nsave_path = \"/kaggle/working/quantized_Llama3.2-3B-Instruct\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Quantized model saved to {save_path}\")\n\n# Save results to CSV\nimport csv\nrounded_tput = round(quant_tput, 1)\nppl = round(ppl, 2)\n\nwith open(\"result.csv\", mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Id\", \"value\"])\n    writer.writerow([0, ppl])\n    writer.writerow([1, rounded_tput])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:12:57.295321Z","iopub.execute_input":"2025-06-01T08:12:57.295622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Warm Up...:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b35d01cd0a39410a860ca9725a76a029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"756621e814e5462b8c52ddccc0687ee0"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 59.50956157357115 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80abc695249b47e9bcddf5dfc812a49e"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.552933131262584 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97272559381443e08349622013a8be8c"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 58.42110772883901 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7c9f0315684eecb17570e580622dc2"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.88736920502202 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d8577a04ea429797302d10dfff38e6"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.72583523384663 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a30ea392f7864c01b1a912a38fa31166"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.70575831781925 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd4a5cbbe61499295f56137cc55c24a"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.676453355498694 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d29b56379b4e7f957f38ccc95ec528"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.62297349367761 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43edbb0aa4440e9b9415d7997284968"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.68416842440063 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c78ccb7c259248d997971912267df9e6"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.66890699711332 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008cf91c8cfe4ffaba32ce079480107e"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 58.05566020249245 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f01a56d25974a079f037d20bfe34cb5"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 58.37503408007185 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0531756ccb7641319889027b0895b52a"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.99622330523192 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59835ee788bd44faada7351eb451d0a5"}},"metadata":{}},{"name":"stdout","text":"Prompt: How to learn a new language?\nResponse:  Here are some effective ways to learn a new language:\n1. **Immerse yourself in the language**: Listen to music, watch TV shows and movies, read books and newspapers, and speak with native speakers.\n2. **Set achievable goals**: Break down your learning process into smaller, manageable goals, such as learning a certain number of words or phrases each day.\n3. **Practice consistently**: Make language learning a regular part of your daily routine, even if it's just for a few minutes each day.\n4. **Use language learning apps**: There are many apps, such as Duolingo, Babbel, and Rosetta Stone, that offer interactive lessons and exercises to help you learn a new language.\n5. **Find a language exchange partner**: Practice speaking with a native speaker or someone who is fluent in the language you want to learn.\n6. **Focus on grammar and vocabulary**: Understanding the grammar and building a strong vocabulary are essential to language learning.\n7. **Use flashcards**: Create flashcards to help you memorize new words and their meanings.\n8. **Take a class or get a tutor**: Consider taking a class or working with a tutor to get personalized feedback and guidance.\n9. **Use spaced repetition**: Review words and phrases at increasingly longer\nThroughput: 57.71368066567376 toks/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2178e865dedc495491e6f0edd2cd94e3"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# PEFT LoRA","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /train_qlora.py\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nimport torch\n\nmodel_path = \"/kaggle/working/quantized_Llama3.2-3B-Instruct\"\n# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\noutput_dir = \"qlora-wikitext2\"\n\n# Load dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\n# Tokenize dataset\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    # return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=192)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from hqq.models.hf.base import AutoHQQHFModel\nmodel = AutoHQQHFModel.from_quantized(\"/kaggle/working/hqq_Llama3.2-3B-Instruct\")\n\nmodel = prepare_model_for_kbit_training(model)\n\n\n# Apply QLoRA config\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./lora-llama3\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    warmup_steps=10,\n    logging_steps=5,\n    save_strategy=\"epoch\",\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    fp16=True,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Upload & Output","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login, create_repo, upload_folder\n\nlogin(\"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\")\n\nrepo_name = \"BrianGodd/hqq-llama3-3b\"\nlocal_model_dir = \"/kaggle/working/hqq_Llama3.2-3B-Instruct\"\n\n# 上傳整個資料夾\nupload_folder(\n    repo_id=repo_name,\n    folder_path=local_model_dir,\n    commit_message=\"upload HQQ quantized Llama3.2-3B\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\n    base_name=\"quantized_Llama3.2-3B-Instruct\",\n    format=\"zip\",\n    root_dir=\"/kaggle/working/quantized_Llama3.2-3B-Instruct\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working\")\n\nfrom IPython.display import FileLink\n\nFileLink(r'quantized_Llama3.2-3B-Instruct.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}