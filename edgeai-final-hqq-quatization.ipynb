{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"03132634575b423ca1da94c4b6dbe483":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038e31c4412d460ea453085dbdc42855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2929bc50a904c96b62e66cb4e048740","IPY_MODEL_e7db96dda6584ebbb734c8b8ab2bf123","IPY_MODEL_4d7bee1cefbc4025948c17dad9836708"],"layout":"IPY_MODEL_dc71ebdf35354ccaa97b5d9e94c37cdc"}},"05212cf48b514730b2f67626b242c8fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"063f8c8ae46a49d68e7bf224eeaa380b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"078607ce0baa425c90238f5f92b69421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b1b59830370497daf32c9252cf50064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_932594c5c14e402d9cedaec281e94aef","max":113,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c60b4c45950448f6a3ae4f6817f83392","value":113}},"10110ec891354744a2d21ba4c661c99c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11296f81ef4a4da7bf70fa560e768792":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_117d41f8d8f343929dc87ef934348f9f","placeholder":"​","style":"IPY_MODEL_32f66ac4387e4d91a1a60ff6807a51a5","value":"100%"}},"117d41f8d8f343929dc87ef934348f9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ad535761b6f471cb253357dea200b52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d3fe58a7e1e4bee921fb9af5f92a182":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f3e5c6c7e9b420c971499420194db2c","placeholder":"​","style":"IPY_MODEL_8f418be5b346452681c0cf9b43b82eb5","value":" 5/5 [00:28&lt;00:00,  4.09s/it]"}},"1f5fce2beedb4c3b8fa48643edb36885":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0644ef0ee254cde8f6a84b3aaa0b93f","placeholder":"​","style":"IPY_MODEL_3f69d8fb25bc416b9bf24d35548c378b","value":"Warm Up...: 100%"}},"203cbd51901c4689b18f23c544deab83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f5fce2beedb4c3b8fa48643edb36885","IPY_MODEL_a7a26b5bf98141b4bf9450638e4c7d9d","IPY_MODEL_1d3fe58a7e1e4bee921fb9af5f92a182"],"layout":"IPY_MODEL_ee2405b605a7403cbe6dbd7e5cea75c7"}},"231f8366e9a54985872443ccd3e1efd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03132634575b423ca1da94c4b6dbe483","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3457516cf88e4386a985f89960ae77cb","value":49}},"32f66ac4387e4d91a1a60ff6807a51a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3457516cf88e4386a985f89960ae77cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b85994d52934168bc7f250563908082":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0bc49c604444a79f5637a6fef8f97a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f69d8fb25bc416b9bf24d35548c378b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41cadea41bdb4385baae9e8a4c9f3cad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f21f2c1f35e84fb080b16de167ee72d0","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5214e4b0fd484be2b442a30f8639cb1d","value":10}},"430f733890644aa7b8ebdebe516a69b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7675468e66c4c65bec33dfbea575cbe","placeholder":"​","style":"IPY_MODEL_83df4d4852984f9fa806ff6f727fa94c","value":"100%"}},"4d7bee1cefbc4025948c17dad9836708":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_702b6f0c8b5743f78ca71554904ce8bb","placeholder":"​","style":"IPY_MODEL_078607ce0baa425c90238f5f92b69421","value":" 625/625 [00:50&lt;00:00, 13.13it/s]"}},"5214e4b0fd484be2b442a30f8639cb1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52d57fb1aeba4990a8a9093a5183868e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b85b3c0bc6e4e9b97c257a9097b8421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bea1178f5b4c4828a4a9ec1fb344be8a","IPY_MODEL_41cadea41bdb4385baae9e8a4c9f3cad","IPY_MODEL_dab1da798b794ee1a4d579db8a5bc791"],"layout":"IPY_MODEL_ddc44bd787824ccea5f48868c50a98f0"}},"636bb2de0fc1489f8f65d7a1a7fa1152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11296f81ef4a4da7bf70fa560e768792","IPY_MODEL_231f8366e9a54985872443ccd3e1efd1","IPY_MODEL_9462f79b89d64e61989bc7cb928bd362"],"layout":"IPY_MODEL_b06c9b27a58b481db958e8afae289436"}},"68fdab016c0d42c58380957d7040f6b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"702b6f0c8b5743f78ca71554904ce8bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72e58936d88d4d5cb204dbd8888751dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820c1773a9794f58a37ccbad81dcbf6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83df4d4852984f9fa806ff6f727fa94c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86baddd4477d4d2fa940256493b0d617":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b6e343e907a4851abf0b48acc2e87d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f3e5c6c7e9b420c971499420194db2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f418be5b346452681c0cf9b43b82eb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"901174c3825e4df3a885588f8a067726":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932594c5c14e402d9cedaec281e94aef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9462f79b89d64e61989bc7cb928bd362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86baddd4477d4d2fa940256493b0d617","placeholder":"​","style":"IPY_MODEL_db34b65b467d4f6ab10437a75a864814","value":" 49/49 [00:00&lt;00:00, 178.90it/s]"}},"97978619758e4f32b4e2d4341c9b9237":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97e023bd82be4f20b14ee0dafd33a1f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9faf4b2c10fa4064b45ec195958efbe2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97e023bd82be4f20b14ee0dafd33a1f1","placeholder":"​","style":"IPY_MODEL_eade00ff8c5e462295fe936051ad5ac8","value":" 113/113 [00:03&lt;00:00, 31.51it/s]"}},"a2929bc50a904c96b62e66cb4e048740":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad3b50c2c04946769e0fcafa25b25ed7","placeholder":"​","style":"IPY_MODEL_063f8c8ae46a49d68e7bf224eeaa380b","value":"100%"}},"a4875f2d29e64f5790cb2bcb5c55e322":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deeca1f999624af487aa3b49db181737","IPY_MODEL_b242476478d741978cbcfb266c519b49","IPY_MODEL_d5859f08bd0e43d09023735b119617b2"],"layout":"IPY_MODEL_3f0bc49c604444a79f5637a6fef8f97a"}},"a7a26b5bf98141b4bf9450638e4c7d9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4e865dbde39492dab21590d1c7ac960","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d00003ef73bc467bae7457c2816fd231","value":5}},"ad3b50c2c04946769e0fcafa25b25ed7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b06c9b27a58b481db958e8afae289436":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b242476478d741978cbcfb266c519b49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf04a1a320cd4aaa88ac8af8d39d33f2","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10110ec891354744a2d21ba4c661c99c","value":0}},"b4e865dbde39492dab21590d1c7ac960":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7675468e66c4c65bec33dfbea575cbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bea1178f5b4c4828a4a9ec1fb344be8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_901174c3825e4df3a885588f8a067726","placeholder":"​","style":"IPY_MODEL_8b6e343e907a4851abf0b48acc2e87d2","value":"Test Inference: 100%"}},"bf04a1a320cd4aaa88ac8af8d39d33f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60b4c45950448f6a3ae4f6817f83392":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6422f5c6dcf46eebdf02c984e3ef94d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_430f733890644aa7b8ebdebe516a69b6","IPY_MODEL_0b1b59830370497daf32c9252cf50064","IPY_MODEL_9faf4b2c10fa4064b45ec195958efbe2"],"layout":"IPY_MODEL_3b85994d52934168bc7f250563908082"}},"ca09d6af91b8407e9afad018818b4972":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00003ef73bc467bae7457c2816fd231":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0644ef0ee254cde8f6a84b3aaa0b93f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5859f08bd0e43d09023735b119617b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05212cf48b514730b2f67626b242c8fe","placeholder":"​","style":"IPY_MODEL_97978619758e4f32b4e2d4341c9b9237","value":" 0/5 [06:18&lt;?, ?it/s]"}},"dab1da798b794ee1a4d579db8a5bc791":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d57fb1aeba4990a8a9093a5183868e","placeholder":"​","style":"IPY_MODEL_820c1773a9794f58a37ccbad81dcbf6d","value":" 10/10 [00:28&lt;00:00,  2.84s/it]"}},"db34b65b467d4f6ab10437a75a864814":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc71ebdf35354ccaa97b5d9e94c37cdc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddc44bd787824ccea5f48868c50a98f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deeca1f999624af487aa3b49db181737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e58936d88d4d5cb204dbd8888751dc","placeholder":"​","style":"IPY_MODEL_68fdab016c0d42c58380957d7040f6b1","value":"Warm Up...:   0%"}},"e7db96dda6584ebbb734c8b8ab2bf123":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca09d6af91b8407e9afad018818b4972","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ad535761b6f471cb253357dea200b52","value":625}},"eade00ff8c5e462295fe936051ad5ac8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2405b605a7403cbe6dbd7e5cea75c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f21f2c1f35e84fb080b16de167ee72d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":419495,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341987,"modelId":363308}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part1: Quantize Llama-3.2-3B-Instruct","metadata":{"id":"2PAW45yCMAM9"}},{"cell_type":"code","source":"!pip3 install huggingface-hub[cli]\n!pip3 install transformers==4.50.3\n!pip3 install torch torchvision torchaudio\n!pip3 install timm==1.0.15\n!pip3 install datasets==3.5.0\n!pip3 install accelerate==1.6.0\n!pip3 install gemlite==0.4.4\n!pip3 install hqq==0.2.5\n!pip3 install triton==3.2.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwWC3xDBlqzE","outputId":"27daf44e-8511-407d-e529-e2010e9b9292","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import float16\nfrom tqdm.auto import tqdm\n\nfrom typing import Union, Callable\nfrom functools import partial\nimport json\nimport timm\nimport os\n\nfrom hqq.core.quantize import HQQLinear\nfrom hqq.core.utils import cleanup\n\nfrom hqq.models.base import (\n    forward_device_hooked,\n    get_all_children_from_model,\n    find_parent,\n    is_leaf_module,\n    BaseHQQModel,\n    BasePatch\n)\n\nfrom hqq.models.hf.base import BaseHQQHFModel\n\n_QUANT_LAYERS = [nn.Linear, HQQLinear]\n_IGNORE_LINEAR = ['lm_head']\n\ndef get_size_of_model(model):\n    size_in_bytes = 0\n    for _, module in model.named_modules():\n        if isinstance(module, HQQLinear):\n            # W_q / Scale / Zero / Bias\n            size_in_bytes += module.W_q.numel() * module.W_q.element_size()\n            size_in_bytes += module.meta['scale'].numel() * module.meta['scale'].element_size()\n            size_in_bytes += module.meta['zero'].numel() * module.meta['zero'].element_size()\n\n            if isinstance(getattr(module, 'bias'), torch.Tensor):\n                size_in_bytes += module.bias.numel() * module.bias.element_size()\n\n        elif is_leaf_module(module):\n            for param in module.parameters():\n                size_in_bytes += param.numel() * param.element_size()\n            for buffer in module.buffers():\n                size_in_bytes += buffer.numel() * buffer.element_size()\n\n    return size_in_bytes\n\n# Get all linear tags available\ndef get_linear_tags_from_model(model, ignore: list) -> list:\n    linear_tags = set()\n    for name, module in model.named_modules():\n        if (type(module) in _QUANT_LAYERS) and (name.split(\".\")[-1] not in ignore):\n            linear_tags.add(name)\n    return list(linear_tags)\n\nclass CustomPatch(BasePatch):\n    # This method iterates through layers of the model that are nn.Linear and processes them via new_nodule = patch_fct(module, params)\n    @classmethod\n    def patch_linearlayers(\n        cls,\n        model,\n        patch_fct: Callable,\n        patch_params: Union[dict, None],\n        verbose: bool = True,\n    ) -> None:\n        ignore_tags = cls.get_ignore_layers(model)\n\n        tmp_mapping = {}\n        for name, module in model.named_modules():\n            if (type(module) in _QUANT_LAYERS) and (name not in ignore_tags):\n                tmp_mapping[name] = module\n\n        for name in tqdm(tmp_mapping, disable=not verbose):\n            linear_tag = name\n            patch_param = (\n                patch_params[linear_tag] if (linear_tag in patch_params) else None\n            )\n            setattr(\n                find_parent(model, name),\n                name.split(\".\")[-1],\n                patch_fct(tmp_mapping[name], patch_param),\n            )\n\n        cleanup()\n\n        # These tags are used to specfiy parameters of the patching in patch_linearlayers()\n    @classmethod\n    def set_auto_linear_tags(cls, model, ignore: list = _IGNORE_LINEAR) -> None:\n        if hasattr(model, \"linear_tags\") is False:\n            linear_tags = cls.get_linear_tags()\n            model.linear_tags = (\n                linear_tags\n                if len(linear_tags) > 0\n                else get_linear_tags_from_model(model, ignore=ignore)\n            )\n            model.base_class = cls\n\nclass CustomHQQTimmModel(BaseHQQModel):\n    # Create empty model\n    @classmethod\n    def create_model(cls, save_dir, kwargs):\n        with open(cls.get_config_file(save_dir), \"r\") as file:\n            config = json.load(file)\n        model = timm.create_model(\n            config[\"architecture\"] + \".\" + config[\"tag\"], pretrained=False\n        )\n        return model\n\n    # Save model architecture\n    @classmethod\n    def cache_model(cls, model, save_dir):\n        try:\n            os.makedirs(save_dir, exist_ok=True)\n        except Exception as error:\n            print(error)\n\n        with open(cls.get_config_file(save_dir), \"w\") as file:\n            json.dump(model.default_cfg, file)\n\n    # Main function to quantize a model. Basically goes through the linear layers specfied in the patching function and replaces them with HQQLinear\n    @classmethod\n    def quantize_model(\n        cls,\n        model,\n        quant_config: dict,\n        compute_dtype: torch.dtype = float16,\n        device: Union[str, list, dict] = \"cuda\",\n    ):\n        # Check if the model was already quantized\n        if getattr(model, \"hqq_quantized\", False):\n            print(\"Model was already quantized\")\n            return\n\n        # Set linear tags automatically\n        cls.setup_model(model)\n\n        # Use the same quantization config for all linear layers. Use None to skip quantizing a specfic layer.\n        if True in [(key in model.linear_tags) for key in quant_config.keys()]:\n            # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n            patch_params = {key: None for key in model.linear_tags}\n            patch_params.update(quant_config)\n        elif quant_config == {}:\n            patch_params = {key: None for key in model.linear_tags}\n        else:\n            # Same quant_config for all layers\n            patch_params = {k: quant_config for k in model.linear_tags}\n\n        # Get list of all nodes in order\n        all_nodes = get_all_children_from_model(model, [])  # ordered nodes\n        try:\n            # Extract block names: This is following Hugging Face models.\n            num_blocks = (\n                len(model.model.blocks)   # TODO: Modify layers to blocks\n                if hasattr(model, \"model\")\n                else len(model.blocks)\n            )\n            all_blocks = [\"blocks.\" + str(i) for i in range(num_blocks)]\n        except Exception:\n            all_blocks = None\n            print(\n                \"Default model structure not supported. Make sure you feed device as dictionary as {name_block: device}\"\n            )\n\n        if isinstance(\n            device, dict\n        ):  # input as {module block name (str): device (str or torch.device)}\n            device_map = device\n            num_devices = len(set([device_map[k] for k in device_map]))\n            all_blocks = list(device_map.keys())\n\n        node_to_block = {}\n        for node in all_nodes:\n            res = [block for block in all_blocks if (block in node)]\n            node_to_block[node] = res[-1] if (len(res) > 0) else node\n\n        # Set device-map\n        if isinstance(device, str):  # single device as str\n            device_map = {k: device for k in all_blocks + all_nodes}\n            num_devices = 1\n\n        if isinstance(device, list):  # list of devices\n            num_devices = len(device)\n            device_map = {}\n            for node in all_nodes:\n                if \".blocks\" in node:\n                    break\n                device_map[node] = device[0]\n\n            for node in all_nodes[::-1]:\n                if \".blocks\" in node:\n                    break\n                device_map[node] = device[-1]\n\n            step, k = len(all_blocks) // num_devices, 0\n            for i in range(0, len(all_blocks), step):\n                for j in range(i, i + step):\n                    device_map[all_blocks[min(j, len(all_blocks) - 1)]] = device[\n                        min(k, num_devices - 1)\n                    ]\n                k += 1\n\n        # Map nodes to block devices\n        for node in all_nodes:\n            device_map[node] = device_map[node_to_block[node]]\n\n        # print(device_map)\n\n        # We replace the nn.Linear layers with HQQLinear\n        def _patch_linear(linear_layer, quant_config):\n            if type(linear_layer) is HQQLinear:\n                return linear_layer\n\n            current_device = device_map[linear_layer.name]\n            if quant_config is not None:\n                out_module = HQQLinear(\n                    linear_layer,\n                    quant_config,\n                    compute_dtype=compute_dtype,\n                    device=current_device,\n                )\n            else:\n                out_module = linear_layer.to(device=current_device, dtype=compute_dtype)\n\n            out_module.device = current_device\n            return out_module\n\n        def _patch_other(layer):\n            current_device = device_map[layer.name]\n            layer.device = current_device\n            return layer.to(device=current_device, dtype=compute_dtype)\n\n        cls.patch_model(model, _patch_other, _patch_linear, patch_params)\n\n        # Insert device switcher\n        if num_devices > 1:\n            core_model = model if hasattr(model, \"blocks\") else model.model\n\n            # Make sure the input (first node) has the input in the right device during generation\n            input_node_child_name = all_nodes[0].split(\".\")[-1]\n            input_node = getattr(core_model, input_node_child_name)\n            input_node.device = device_map[all_nodes[0]]\n            input_node.forward_orig = input_node.forward\n            input_node.forward = partial(forward_device_hooked, input_node)\n            setattr(core_model, input_node_child_name, input_node)\n\n            # Make sure all inputs to the blocks are in the right device\n            for i in range(len(core_model.blocks)):\n                core_model.blocks[i].device = device_map[core_model.blocks[i].name]\n                core_model.blocks[i].forward_orig = core_model.blocks[i].forward\n                core_model.blocks[i].forward = partial(\n                    forward_device_hooked, core_model.blocks[i]\n                )\n\n        # Set base class\n        model.base_class = cls\n\n        model.hqq_quantized = True\n\n        return model\n\nclass CustomHQQHFModel(BaseHQQHFModel):\n    # Main function to quantize a model. Basically goes through the linear layers specfied in the patching function and replaces them with HQQLinear\n    @classmethod\n    def quantize_model(\n        cls,\n        model,\n        quant_config: dict,\n        compute_dtype: torch.dtype = float16,\n        device: Union[str, list, dict] = \"cuda\",\n    ):\n        # Check if the model was already quantized\n        if getattr(model, \"hqq_quantized\", False):\n            print(\"Model was already quantized\")\n            return\n\n        # Set linear tags automatically\n        cls.setup_model(model)\n\n        # Use the same quantization config for all linear layers. Use None to skip quantizing a specfic layer.\n        if True in [(key in model.linear_tags) for key in quant_config.keys()]:\n            # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n            patch_params = {key: None for key in model.linear_tags}\n            patch_params.update(quant_config)\n        elif quant_config == {}:\n            patch_params = {key: None for key in model.linear_tags}\n        else:\n            # Same quant_config for all layers\n            patch_params = {k: quant_config for k in model.linear_tags}\n\n        # Get list of all nodes in order\n        all_nodes = get_all_children_from_model(model, [])  # ordered nodes\n        try:\n            # Extract block names: This is following Hugging Face models.\n            num_blocks = (\n                len(model.model.layers)\n                if hasattr(model, \"model\")\n                else len(model.layers)\n            )\n            all_blocks = [\"model.layers.\" + str(i) for i in range(num_blocks)]\n        except Exception:\n            all_blocks = None\n            print(\n                \"Default model structure not supported. Make sure you feed device as dictionary as {name_block: device}\"\n            )\n\n        if isinstance(\n            device, dict\n        ):  # input as {module block name (str): device (str or torch.device)}\n            device_map = device\n            num_devices = len(set([device_map[k] for k in device_map]))\n            all_blocks = list(device_map.keys())\n\n        node_to_block = {}\n        for node in all_nodes:\n            res = [block for block in all_blocks if (block in node)]\n            node_to_block[node] = res[-1] if (len(res) > 0) else node\n\n        # Set device-map\n        if isinstance(device, str):  # single device as str\n            device_map = {k: device for k in all_blocks + all_nodes}\n            num_devices = 1\n\n        if isinstance(device, list):  # list of devices\n            num_devices = len(device)\n            device_map = {}\n            for node in all_nodes:\n                if \".layers\" in node:\n                    break\n                device_map[node] = device[0]\n\n            for node in all_nodes[::-1]:\n                if \".layers\" in node:\n                    break\n                device_map[node] = device[-1]\n\n            step, k = len(all_blocks) // num_devices, 0\n            for i in range(0, len(all_blocks), step):\n                for j in range(i, i + step):\n                    device_map[all_blocks[min(j, len(all_blocks) - 1)]] = device[\n                        min(k, num_devices - 1)\n                    ]\n                k += 1\n\n        # Map nodes to block devices\n        for node in all_nodes:\n            device_map[node] = device_map[node_to_block[node]]\n\n        # print(device_map)\n\n        # We replace the nn.Linear layers with HQQLinear\n        def _patch_linear(linear_layer, quant_config):\n            if type(linear_layer) is HQQLinear:\n                return linear_layer\n\n            current_device = device_map[linear_layer.name]\n            if quant_config is not None:\n                out_module = HQQLinear(\n                    linear_layer,\n                    quant_config,\n                    compute_dtype=compute_dtype,\n                    device=current_device,\n                )\n            else:\n                out_module = linear_layer.to(device=current_device, dtype=compute_dtype)\n\n            out_module.device = current_device\n            return out_module\n\n        def _patch_other(layer):\n            current_device = device_map[layer.name]\n            layer.device = current_device\n            return layer.to(device=current_device, dtype=compute_dtype)\n\n        cls.patch_model(model, _patch_other, _patch_linear, patch_params)\n\n        # Insert device switcher\n        if num_devices > 1:\n            core_model = model if hasattr(model, \"layers\") else model.model\n\n            # Make sure the input (first node) has the input in the right device during generation\n            input_node_child_name = all_nodes[0].split(\".\")[-1]\n            input_node = getattr(core_model, input_node_child_name)\n            input_node.device = device_map[all_nodes[0]]\n            input_node.forward_orig = input_node.forward\n            input_node.forward = partial(forward_device_hooked, input_node)\n            setattr(core_model, input_node_child_name, input_node)\n\n            # Make sure all inputs to the blocks are in the right device\n            for i in range(len(core_model.layers)):\n                core_model.layers[i].device = device_map[core_model.layers[i].name]\n                core_model.layers[i].forward_orig = core_model.layers[i].forward\n                core_model.layers[i].forward = partial(\n                    forward_device_hooked, core_model.layers[i]\n                )\n\n        # Set base class\n        model.base_class = cls\n\n        model.hqq_quantized = True\n\n        return model\n\n# Auto class used for HF models if no architecture was manually setup\nclass AutoHQQHFModel(CustomHQQHFModel, CustomPatch):\n    pass\n\nclass AutoHQQTimmModel(CustomHQQTimmModel, CustomPatch):\n    pass","metadata":{"id":"U1WJOPqHlfiA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nfrom tqdm.auto import tqdm\n\nfrom torchvision import datasets, transforms\nfrom timm.data import create_transform\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom torch.utils.data import DataLoader\n\ndef build_transform(is_train):\n    input_size = 224\n    eval_crop_ratio = 1.0\n\n    resize_im = input_size > 32\n    if is_train:\n        # this should always dispatch to transforms_imagenet_train\n        transform = create_transform(\n            input_size=input_size,\n            is_training=True,\n            color_jitter=0.3,\n            auto_augment='rand-m9-mstd0.5-inc1',\n            interpolation='bicubic',\n            re_prob=0.0,\n            re_mode='pixel',\n            re_count=1,\n        )\n        if not resize_im:\n            # replace RandomResizedCropAndInterpolation with\n            # RandomCrop\n            transform.transforms[0] = transforms.RandomCrop(\n                input_size, padding=4)\n        return transform\n\n    t = []\n    if resize_im:\n        size = int(input_size / eval_crop_ratio)\n        t.append(\n            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n        )\n        t.append(transforms.CenterCrop(input_size))\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n    return transforms.Compose(t)\n\ndef build_dataset_CIFAR100(is_train, data_path):\n    transform = build_transform(is_train)\n    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n    nb_classes = 100\n    return dataset, nb_classes\n\ndef prepare_data(batch_size):\n    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n    return train_loader, test_loader, nb_classes\n\ndef evaluate_model(model, data_loader, device):\n    model.to(device)\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    # print(f'Accuracy of the model on the test images: {accuracy}%')\n    return accuracy\n","metadata":{"id":"PoUDSIPzoOu9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from hqq.core.quantize import BaseQuantizeConfig\nfrom huggingface_hub import login\n#or just keyin your token here\nlogin()","metadata":{"id":"92ottf6VrLBu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Throughput: 60.09128305820182 toks/s\n# Perplexity (PPL): 11.403003692626953\ndef get_quant_config_slm(model):\n    quant_config = {}\n\n    n_layers = model.config.num_hidden_layers\n\n    # 定義不同精度等級的量化參數\n    q_very_light = BaseQuantizeConfig(nbits=4, group_size=32)\n    q_base       = BaseQuantizeConfig(nbits=4, group_size=64)\n    q_down_light = BaseQuantizeConfig(nbits=4, group_size=128)  # 專供 down_proj 用\n    q_important  = BaseQuantizeConfig(nbits=8, group_size=64)\n    q_important_middle = BaseQuantizeConfig(nbits=8, group_size=128)\n    q_important_light = BaseQuantizeConfig(nbits=8, group_size=256)\n\n    for i in range(n_layers):\n\n        quant_config[f'model.layers.{i}.self_attn.q_proj'] = q_very_light\n\n        quant_config[f'model.layers.{i}.self_attn.k_proj'] = q_important_light\n        quant_config[f'model.layers.{i}.self_attn.v_proj'] = q_important_light\n\n        quant_config[f'model.layers.{i}.self_attn.o_proj'] = q_important_middle\n\n        quant_config[f'model.layers.{i}.mlp.gate_proj'] = q_down_light\n        quant_config[f'model.layers.{i}.mlp.up_proj'] = q_down_light\n\n        quant_config[f'model.layers.{i}.mlp.down_proj'] = q_down_light\n\n    return quant_config","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\nfrom tqdm.auto import tqdm\nfrom datasets import load_dataset\nimport random\nimport numpy as np\n\nfrom hqq.utils.patching import recommended_inductor_config_setter\n\ndef generate(model, input_ids, past_key_values, max_new_tokens, activate_timing, verbose=True):\n    input_ids = input_ids.clone()\n    tput = None\n    # Run an initial forward pass to compute and store the static KV cache\n    if verbose:\n        print('Prefilling...')\n    with torch.no_grad():\n        # outputs = custom_forward(model, input_ids, past_key_values=past_key_values, use_cache=True, position_ids=None, attention_mask=None, cache_position=None, is_compiled=False)\n        outputs = model.prefill_forward(input_ids, past_key_values=past_key_values, position_ids=None, attention_mask=None, cache_position=None, logits_to_keep=1)\n        past_key_values = outputs.past_key_values\n        next_token = torch.argmax(outputs.logits, dim=-1)\n        input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n    # Generate tokens one by one using a for loop and update the KV cache\n    if verbose:\n        print('Decoding...')\n    with torch.no_grad():\n        if activate_timing:\n            start_event = torch.cuda.Event(enable_timing=True)\n            end_event = torch.cuda.Event(enable_timing=True)\n            start_event.record()\n        for _ in range(max_new_tokens):\n            # Compute position_ids using the current sequence length\n            pos = input_ids.shape[1]\n            cache_position = torch.arange(pos, pos+1, device=input_ids.device, dtype=torch.long)\n\n            # Run the model on the last token using the cached key-value pairs\n            outputs = model(\n                next_token,\n                past_key_values=past_key_values,\n                position_ids=cache_position.unsqueeze(0),\n                cache_position=cache_position\n            )\n            logits = outputs.logits\n\n            # Greedily select the token with the highest probability\n            next_token = torch.argmax(logits, dim=-1)\n\n            # Append the predicted token to the generated sequence\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n            # Update the KV cache for the next iteration\n            past_key_values = outputs.past_key_values\n        if activate_timing:\n            end_event.record()\n        torch.cuda.synchronize()\n    if activate_timing:\n        tput = max_new_tokens / start_event.elapsed_time(end_event) * 1000\n        # print(f\"Throughput: {tput} toks/sec\")\n    return input_ids, tput\n\ndef evaluate_ppl(model, tokenizer, device=\"cuda:0\"):\n    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    # print(f\"Dataset length: {len(test_dataset)}\")\n\n    test_enc = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    model.seqlen = 2048\n    test_enc = test_enc.input_ids.to(device)\n\n    nsamples = test_enc.numel() // model.seqlen\n    nlls = []\n    for i in tqdm(range(nsamples), desc=\"Evaluating...\"):\n        batch = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)]\n\n        with torch.no_grad():\n            lm_logits = model(batch).logits\n\n        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n        shift_labels = test_enc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * model.seqlen\n        nlls.append(neg_log_likelihood)\n\n    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n\n    return ppl.item()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764,"referenced_widgets":["203cbd51901c4689b18f23c544deab83","1f5fce2beedb4c3b8fa48643edb36885","a7a26b5bf98141b4bf9450638e4c7d9d","1d3fe58a7e1e4bee921fb9af5f92a182","ee2405b605a7403cbe6dbd7e5cea75c7","d0644ef0ee254cde8f6a84b3aaa0b93f","3f69d8fb25bc416b9bf24d35548c378b","b4e865dbde39492dab21590d1c7ac960","d00003ef73bc467bae7457c2816fd231","8f3e5c6c7e9b420c971499420194db2c","8f418be5b346452681c0cf9b43b82eb5","5b85b3c0bc6e4e9b97c257a9097b8421","bea1178f5b4c4828a4a9ec1fb344be8a","41cadea41bdb4385baae9e8a4c9f3cad","dab1da798b794ee1a4d579db8a5bc791","ddc44bd787824ccea5f48868c50a98f0","901174c3825e4df3a885588f8a067726","8b6e343e907a4851abf0b48acc2e87d2","f21f2c1f35e84fb080b16de167ee72d0","5214e4b0fd484be2b442a30f8639cb1d","52d57fb1aeba4990a8a9093a5183868e","820c1773a9794f58a37ccbad81dcbf6d","c6422f5c6dcf46eebdf02c984e3ef94d","430f733890644aa7b8ebdebe516a69b6","0b1b59830370497daf32c9252cf50064","9faf4b2c10fa4064b45ec195958efbe2","3b85994d52934168bc7f250563908082","b7675468e66c4c65bec33dfbea575cbe","83df4d4852984f9fa806ff6f727fa94c","932594c5c14e402d9cedaec281e94aef","c60b4c45950448f6a3ae4f6817f83392","97e023bd82be4f20b14ee0dafd33a1f1","eade00ff8c5e462295fe936051ad5ac8","a4875f2d29e64f5790cb2bcb5c55e322","deeca1f999624af487aa3b49db181737","b242476478d741978cbcfb266c519b49","d5859f08bd0e43d09023735b119617b2","3f0bc49c604444a79f5637a6fef8f97a","72e58936d88d4d5cb204dbd8888751dc","68fdab016c0d42c58380957d7040f6b1","bf04a1a320cd4aaa88ac8af8d39d33f2","10110ec891354744a2d21ba4c661c99c","05212cf48b514730b2f67626b242c8fe","97978619758e4f32b4e2d4341c9b9237"]},"id":"5b9yYg18pR2k","outputId":"47e63067-b371-4704-e4e3-d26ee77d8dc1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch._dynamo.reset()\n############## Set Up ##############\ntorch.manual_seed(0)\nrandom.seed(0)\nrecommended_inductor_config_setter()\n\nmax_new_tokens = 256    # Number of new tokens to generate\ndevice = 'cuda:0'\nbackend = 'gemlite'\n\nmodel_name = \"meta-llama/Llama-3.2-3B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=device,\n    token = \"YOUR_TOKEN\"\n)\ntorch.compile(model)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Separate Prefill & Decode Forwarding Function\nmodel.prefill_forward = model.forward\nmodel.forward = torch.compile(model.forward, mode='max-autotune', dynamic=False, fullgraph=True)\n\nprint(f'Model Size Before Quant: {get_size_of_model(model) / (1024 ** 2)} MiB')\n\n# TODO: Quantize\nquant_config = get_quant_config_slm(model)\n\nAutoHQQHFModel.quantize_model(model, quant_config=quant_config, compute_dtype=torch.float16, device=device)\n\nsave_dir = \"/kaggle/working/hqq_Llama3.2-3B-Instruct\"\nAutoHQQHFModel.save_quantized(model, save_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part2: Optimized Inference/Prediction/Eval","metadata":{}},{"cell_type":"code","source":"from hqq.utils.patching import prepare_for_inference\nprepare_for_inference(model, backend=backend)\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warmup_prompt = \"Explain what AI is.\"\ninput_ids = tokenizer(warmup_prompt, return_tensors=\"pt\").input_ids.to(device)\npast_key_values = StaticCache(\n    config=model.config,\n    max_batch_size=1,\n    max_cache_len=max_new_tokens + 16,\n    device=model.device,\n    dtype=torch.float16\n)\nfor i in tqdm(range(5), desc=\"Warm Up...\"):\n    generated = generate(model, input_ids, past_key_values, max_new_tokens, activate_timing=False, verbose=False)\n    past_key_values.reset()\n\nprompt = \"How to learn a new language?\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n\ntputs = []\nfor _ in tqdm(range(10), desc=\"Test Inference\"):\n    generated, tput = generate(model, input_ids, past_key_values, max_new_tokens, activate_timing=True, verbose=False)\n    past_key_values.reset()\n    tputs.append(tput)\nresponse = tokenizer.decode(generated[0][input_ids.shape[1]:], skip_special_tokens=True)\ntputs = np.sort(tputs)[2:-2]\nquant_tput = np.mean(tputs)\nprint(f'Prompt: {prompt}\\nResponse: {response}\\nThroughput: {quant_tput} toks/s')\n\nprint(f'Model Size After Quant: {get_size_of_model(model) / (1024 ** 2)} MiB')\n\nppl = evaluate_ppl(model, tokenizer, device)\nprint(f\"Perplexity (PPL): {ppl}\")\n# print(f\"Speedup: {quant_tput / org_tput} x\")\n\nscore = 0\nscore += 10 if quant_tput >= 31.0 else 0\nscore += 30 if quant_tput >= 54.0 else 0\nif ppl > 11.5:\n    score = 0 \nprint(f'Score: {score}')\n\n# torch.save(model.state_dict(), \"/kaggle/working/llama3_quantized.pth\")\nsave_path = \"/kaggle/working/quantized_Llama3.2-3B-Instruct\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Quantized model saved to {save_path}\")\n\n# Save results to CSV\nimport csv\nrounded_tput = round(quant_tput, 1)\nppl = round(ppl, 2)\n\nwith open(\"result.csv\", mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Id\", \"value\"])\n    writer.writerow([0, ppl])\n    writer.writerow([1, rounded_tput])\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}